{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPm42glOAhRnHP+XeNklRCb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","output_embedded_package_id":"1XUl15fOUcIUvJQpf7d_KBSuhhVNpUMHc"},"id":"c3ilHwU7-0yf","executionInfo":{"status":"ok","timestamp":1689335927191,"user_tz":-360,"elapsed":3746347,"user":{"displayName":"Tanmoy Mazumder","userId":"07166002203080382723"}},"outputId":"f8de280d-9237-4391-aa8e-678a4fce7eb3"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader\n","from tqdm import tqdm\n","import torchvision\n","\n","# Set device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Define Generator and Discriminator networks\n","class Generator(nn.Module):\n","    def __init__(self, input_size, output_size):\n","        super(Generator, self).__init__()\n","        self.model = nn.Sequential(\n","            nn.Linear(input_size, 256),\n","            nn.LeakyReLU(0.2),\n","            nn.Linear(256, 512),\n","            nn.LeakyReLU(0.2),\n","            nn.Linear(512, 1024),\n","            nn.LeakyReLU(0.2),\n","            nn.Linear(1024, output_size),\n","            nn.Tanh()\n","        )\n","\n","    def forward(self, x):\n","        return self.model(x)\n","\n","class Discriminator(nn.Module):\n","    def __init__(self, input_size):\n","        super(Discriminator, self).__init__()\n","        self.model = nn.Sequential(\n","            nn.Linear(input_size, 512),\n","            nn.LeakyReLU(0.2),\n","            nn.Linear(512, 256),\n","            nn.LeakyReLU(0.2),\n","            nn.Linear(256, 1),\n","            nn.Sigmoid()\n","        )\n","\n","    def forward(self, x):\n","        return self.model(x)\n","\n","# Hyperparameters\n","input_size = 100\n","batch_size = 64\n","num_epochs = 30\n","learning_rate = 0.0002\n","\n","# Load MNIST dataset\n","transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5,), (0.5,))\n","])\n","train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","\n","# Initialize networks and optimizers\n","generator = Generator(input_size, 784).to(device)\n","discriminator = Discriminator(784).to(device)\n","optimizer_G = optim.Adam(generator.parameters(), lr=learning_rate)\n","optimizer_D = optim.Adam(discriminator.parameters(), lr=learning_rate)\n","\n","# Loss function\n","criterion = nn.BCELoss()\n","\n","# Training loop\n","total_steps = len(train_loader)\n","for epoch in range(num_epochs):\n","    for i, (real_images, _) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")):\n","        real_images = real_images.view(-1, 784).to(device)\n","        batch_size = real_images.size(0)\n","\n","        # Training Discriminator\n","        optimizer_D.zero_grad()\n","        real_labels = torch.ones(batch_size, 1).to(device)\n","        fake_labels = torch.zeros(batch_size, 1).to(device)\n","\n","        # Real images\n","        real_outputs = discriminator(real_images)\n","        real_loss = criterion(real_outputs, real_labels)\n","\n","        # Fake images\n","        z = torch.randn(batch_size, input_size).to(device)\n","        fake_images = generator(z)\n","        fake_outputs = discriminator(fake_images.detach())\n","        fake_loss = criterion(fake_outputs, fake_labels)\n","\n","        # Total loss\n","        d_loss = real_loss + fake_loss\n","        d_loss.backward()\n","        optimizer_D.step()\n","\n","        # Training Generator\n","        optimizer_G.zero_grad()\n","        fake_outputs = discriminator(fake_images)\n","        g_loss = criterion(fake_outputs, real_labels)\n","        g_loss.backward()\n","        optimizer_G.step()\n","\n","        # Update progress bar\n","        tqdm.write(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{total_steps}], \"\n","                   f\"Discriminator Loss: {d_loss.item():.4f}, Generator Loss: {g_loss.item():.4f}\")\n","\n","# Generate and save sample images\n","z = torch.randn(16, input_size).to(device)\n","generated_images = generator(z)\n","torchvision.utils.save_image(generated_images.view(generated_images.size(0), 1, 28, 28),\n","                             \"generated_images.png\", nrow=4, normalize=True)\n","\n","# Generate image based on user input\n","z_input = torch.randn(1, input_size).to(device)\n","with torch.no_grad():\n","    generated_image = generator(z_input)\n","torchvision.utils.save_image(generated_image.view(1, 1, 28, 28),\n","                             f\"generated_image.png\", normalize=True)"]}]}