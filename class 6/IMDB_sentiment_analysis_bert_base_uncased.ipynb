{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxXYeagmLjzr"
      },
      "source": [
        "\n",
        "source of tutorial: https://skimai.com/fine-tuning-bert-for-sentiment-analysis/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GmAGlnlWmw8m",
        "outputId": "c0402c3e-3b42-4efb-cffa-d7eae42d5bee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LF9s_oH1LaMf",
        "outputId": "3e324ffc-25a4-4a8f-8b31-cdb7e44081c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 GPU(s) available.\n",
            "Device name: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
        "    print('Device name:', torch.cuda.get_device_name(0))\n",
        "\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SkOTiCxuPJxV",
        "outputId": "12bcf015-5408-4597-d45d-18185c92a143"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.21.2-py3-none-any.whl (4.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.7 MB 35.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.9.1-py3-none-any.whl (120 kB)\n",
            "\u001b[K     |████████████████████████████████| 120 kB 77.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 53.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.9.1 tokenizers-0.12.1 transformers-4.21.2\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CDv2btpSO2rA"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, roc_curve, auc\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import BertModel\n",
        "from transformers import BertTokenizer\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aw485xy6ONvc",
        "outputId": "4a980655-bd29-45d6-8f20-6f7b5ff9a616"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "#48s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "T5uAvTIgMZzn",
        "outputId": "3ef0737f-e0ef-4cd8-bcf7-58de8e22c39c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                  review sentiment\n",
              "6476   Great screenplay and some of the best actors t...  positive\n",
              "46974  On the face of it, Ruiz has set out to make a ...  negative\n",
              "25156  This is a dumb movie. Maybe my judgment wouldn...  negative"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fa5f4fd4-e531-4a9a-852c-2227516b6167\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6476</th>\n",
              "      <td>Great screenplay and some of the best actors t...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46974</th>\n",
              "      <td>On the face of it, Ruiz has set out to make a ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25156</th>\n",
              "      <td>This is a dumb movie. Maybe my judgment wouldn...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fa5f4fd4-e531-4a9a-852c-2227516b6167')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-fa5f4fd4-e531-4a9a-852c-2227516b6167 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-fa5f4fd4-e531-4a9a-852c-2227516b6167');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "data = pd.read_csv('/content/drive/Shareddrives/UAP-NLP Group/Materials/notebooks_and_dataset/IMDB Dataset.csv')\n",
        "data.sample(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "8mzYXOF2SVgw",
        "outputId": "8e4b1b8a-5ab3-40f4-9409-f11f1520f725"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                 review     label\n",
              "4016  I just finished watching Dog Watch. I thought ...  negative\n",
              "4899  What a strangely wonderful, if sometimes sligh...  positive"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-88f6bee9-e090-464a-9d68-64bdbe543353\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4016</th>\n",
              "      <td>I just finished watching Dog Watch. I thought ...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4899</th>\n",
              "      <td>What a strangely wonderful, if sometimes sligh...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-88f6bee9-e090-464a-9d68-64bdbe543353')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-88f6bee9-e090-464a-9d68-64bdbe543353 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-88f6bee9-e090-464a-9d68-64bdbe543353');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "data.rename(columns = {'sentiment':'label'}, inplace = True)\n",
        "data.sample(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4LY4zF77RkEu"
      },
      "outputs": [],
      "source": [
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = data.review.values\n",
        "y = data.label.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rp_eGPHMSg5f"
      },
      "outputs": [],
      "source": [
        "y = list(pd.get_dummies(y, drop_first= True)['positive'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hd0ve7NsTSw_"
      },
      "outputs": [],
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.20, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvCZ8wYzVkmY"
      },
      "source": [
        "**2. Tokenization and Input Formatting**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Before tokenizing our text, we will perform some slight processing on our text including removing entity mentions (eg. @united) and some special character. The level of processing here is much less than in previous approachs because BERT was trained with the entire sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMqB4KyOVukX"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "def text_preprocessing(text):\n",
        "    \"\"\"\n",
        "    - Remove entity mentions (eg. '@united')\n",
        "    - Correct errors (eg. '&amp;' to '&')\n",
        "    @param    text (str): a string to be processed.\n",
        "    @return   text (Str): the processed string.\n",
        "    \"\"\"\n",
        "    # Remove '@name'\n",
        "    text = re.sub(r'(@.*?)[\\s]', ' ', text)\n",
        "\n",
        "    # Replace '&amp;' with '&'\n",
        "    text = re.sub(r'&amp;', '&', text)\n",
        "\n",
        "    # Remove trailing whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kd9RMZCPQA4H"
      },
      "source": [
        "***2.1. BERT Tokenizer***\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "In order to apply the pre-trained BERT, we must use the tokenizer provided by the library. This is because (1) the model has a specific, fixed vocabulary and (2) the BERT tokenizer has a particular way of handling out-of-vocabulary words.\n",
        "\n",
        "In addition, we are required to add special tokens to the start and end of each sentence, pad & truncate all sentences to a single constant length, and explicitly specify what are padding tokens with the \"attention mask\".\n",
        "\n",
        "The encode_plus method of BERT tokenizer will:\n",
        "\n",
        "(1) split our text into tokens,\n",
        "\n",
        "(2) add the special [CLS] and [SEP] tokens, and\n",
        "\n",
        "(3) convert these tokens into indexes of the tokenizer vocabulary,\n",
        "\n",
        "(4) pad or truncate sentences to max length, and\n",
        "\n",
        "(5) create attention mask."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "2d0a50fb230e4fc7a4f391eb0efb55c0",
            "2f10f02cd1904522b3ad5c5a31e8bc06",
            "24f940e557f4499dbf9de70b4e64de4f",
            "987baedbdabf4a29b42379ad8126473b",
            "50002f23763a41b38e7188be9b8623fb",
            "19d9d15ebe2d46c1ac10c6b6b3157060",
            "9b2ee64f724344fbad3ad380376a5f10",
            "d6da59d2b553438781e585250d4edb27",
            "a40667a6174443bda521e26b026d06ec",
            "59664a7d9f864137b3013f2699492664",
            "4b1e622beff84bf8b9e64df70bb8f196",
            "4c720ea55a0b497dbf3c537cb1c1e227",
            "d153a9fefd654ba394939a17553bf787",
            "41811c0955924f499246b15a4146130a",
            "db34ef20b2c443778441d6b20a560f5d",
            "5b596fdf7af4446d93b890f53bca933e",
            "9a396d0b3bfe4584a455fa3e4422fc5e",
            "10cb97156ddf4d539e2ad91dbc2d436c",
            "5c3c4ddfa3424bf4b3516242884db39a",
            "1146f00b3fa943529b86e73fd477f82b",
            "6cf5cbc5be0e4798844f33012bd6e34e",
            "7370701388004662b38c44a082d92cbe",
            "a74ae563f0ca4af5acd1f3e1023dccf1",
            "159606e3bfd647d38aaa525735d49a7c",
            "e95aaa1b514848228ac17c258af9d2b3",
            "e97773bf80d84d7a90350be8dd0ecbaf",
            "8976e442c5274abc85076e3738848c61",
            "cf63af6978f645ce88f028db431116f0",
            "d84fc750d9a34383b64120bdfc9ef299",
            "e7b0d94931544c88ba84a1da23ed3758",
            "60a83d8f267147e9be3d803dff3d25ef",
            "f483cbf1d8d047ecac46ef899894cdaa",
            "dbc1ac5b575d43f5868adeb1538ab94a"
          ]
        },
        "id": "tCRFrHDSOzdl",
        "outputId": "b83692e6-636d-4f79-90b3-9206cf349c38"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading vocab.txt:   0%|          | 0.00/226k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2d0a50fb230e4fc7a4f391eb0efb55c0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4c720ea55a0b497dbf3c537cb1c1e227"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a74ae563f0ca4af5acd1f3e1023dccf1"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Load the BERT tokenizer\n",
        "#from transformers import DistilBertTokenizerFast, DistilBertModel\n",
        "#tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "# Create a function to tokenize a set of texts\n",
        "def preprocessing_for_bert(data):\n",
        "    \"\"\"Perform required preprocessing steps for pretrained BERT.\n",
        "    @param    data (np.array): Array of texts to be processed.\n",
        "    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n",
        "    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n",
        "                  tokens should be attended to by the model.\n",
        "    \"\"\"\n",
        "    # Create empty lists to store outputs\n",
        "    input_ids = []\n",
        "    attention_masks = []\n",
        "\n",
        "    # For every sentence...\n",
        "    for sent in data:\n",
        "        # `encode_plus` will:\n",
        "        #    (1) Tokenize the sentence\n",
        "        #    (2) Add the `[CLS]` and `[SEP]` token to the start and end\n",
        "        #    (3) Truncate/Pad sentence to max length\n",
        "        #    (4) Map tokens to their IDs\n",
        "        #    (5) Create attention mask\n",
        "        #    (6) Return a dictionary of outputs\n",
        "        encoded_sent = tokenizer.encode_plus(\n",
        "            text=text_preprocessing(sent),  # Preprocess sentence\n",
        "            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
        "            max_length=MAX_LEN,                  # Max length to truncate/pad\n",
        "            truncation = True,\n",
        "            pad_to_max_length=True,         # Pad sentence to max length\n",
        "            #return_tensors='pt',           # Return PyTorch tensor\n",
        "            return_attention_mask=True      # Return attention mask\n",
        "            )\n",
        "\n",
        "        # Add the outputs to the lists\n",
        "        input_ids.append(encoded_sent.get('input_ids'))#They are token indices, numerical representations of tokens building the sequences that will be used as input by the model.\n",
        "        attention_masks.append(encoded_sent.get('attention_mask'))#Attention masks allow us to send a batch into the transformer even when the examples in the batch have varying lengths.\n",
        "\n",
        "    # Convert lists to tensors\n",
        "    input_ids = torch.tensor(input_ids)\n",
        "    attention_masks = torch.tensor(attention_masks)\n",
        "\n",
        "    return input_ids, attention_masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k7EvsQ--XijU"
      },
      "outputs": [],
      "source": [
        "from google.colab import output\n",
        "output.disable_custom_widget_manager()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NoyxTAbfRQ7w",
        "outputId": "613f0bff-8d70-4580-b4b2-c3a687a861c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (563 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[101, 2028, 1997, 1996, 2060, 15814, 2038, 3855, 2008, 2044, 3666, 2074, 1015, 11472, 2792, 2017, 1005, 2222, 2022, 13322, 1012, 2027, 2024, 2157, 1010, 2004, 2023, 2003, 3599, 2054, 3047, 2007, 2033, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1996, 2034, 2518, 2008, 4930, 2033, 2055, 11472, 2001, 2049, 24083, 1998, 4895, 10258, 2378, 8450, 5019, 1997, 4808, 1010, 2029, 2275, 1999, 2157, 2013, 1996, 2773, 2175, 1012, 3404, 2033, 1010, 2023, 2003, 2025, 1037, 2265, 2005, 1996, 8143, 18627, 2030, 5199, 3593, 1012, 2023, 2265, 8005, 2053, 17957, 2007, 12362, 2000, 5850, 1010, 3348, 2030, 4808, 1012, 2049, 2003, 13076, 1010, 1999, 1996, 4438, 2224, 1997, 1996, 2773, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2009, 2003, 2170, 11472, 2004, 2008, 2003, 1996, 8367, 2445, 2000, 1996, 17411, 4555, 3036, 2110, 7279, 4221, 12380, 2854, 1012, 2009, 7679, 3701, 2006, 14110, 2103, 1010, 2019, 6388, 2930, 1997, 1996, 3827, 2073, 2035, 1996, 4442, 2031, 3221, 21430, 1998, 2227, 20546, 2015, 1010, 2061, 9394, 2003, 2025, 2152, 2006, 1996, 11376, 1012, 7861, 2103, 2003, 2188, 2000, 2116, 1012, 1012, 26030, 2015, 1010, 7486, 1010, 18542, 10230, 1010, 7402, 2015, 1010, 8135, 1010, 16773, 1010, 3493, 1998, 2062, 1012, 1012, 1012, 1012, 2061, 8040, 16093, 28331, 1010, 2331, 14020, 1010, 26489, 6292, 24069, 1998, 22824, 10540, 2024, 2196, 2521, 2185, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1045, 2052, 2360, 1996, 2364, 5574, 1997, 1996, 2265, 2003, 2349, 2000, 1996, 2755, 2008, 2009, 3632, 2073, 2060, 3065, 2876, 1005, 1056, 8108, 1012, 5293, 3492, 4620, 4993, 2005, 7731, 9501, 1010, 5293, 11084, 1010, 5293, 7472, 1012, 1012, 1012, 11472, 2987, 1005, 1056, 6752, 2105, 1012, 1996, 2034, 2792, 1045, 2412, 2387, 4930, 2033, 2004, 2061, 11808, 2009, 2001, 16524, 1010, 1045, 2481, 1005, 1056, 2360, 1045, 2001, 3201, 2005, 2009, 1010, 2021, 2004, 1045, 3427, 2062, 1010, 1045, 2764, 1037, 5510, 2005, 11472, 1010, 1998, 2288, 17730, 2000, 1996, 2152, 3798, 1997, 8425, 4808, 1012, 2025, 2074, 4808, 1010, 2021, 21321, 1006, 15274, 4932, 2040, 1005, 2222, 2022, 2853, 2041, 2005, 1037, 15519, 1010, 13187, 2040, 1005, 2222, 3102, 2006, 2344, 1998, 2131, 2185, 2007, 2009, 1010, 2092, 5450, 2098, 1010, 2690, 2465, 13187, 2108, 2357, 2046, 3827, 7743, 2229, 2349, 2000, 2037, 3768, 1997, 2395, 4813, 2030, 3827, 3325, 1007, 3666, 11472, 1010, 2017, 2089, 2468, 6625, 2007, 2054, 2003, 8796, 10523, 1012, 1012, 1012, 1012, 2008, 2015, 2065, 2017, 2064, 2131, 1999, 3543, 2007, 2115, 9904, 2217, 1012, 102]\n",
            "Max length:  3157\n"
          ]
        }
      ],
      "source": [
        "# Encode our concatenated data\n",
        "all_reviews = data.review.values\n",
        "\n",
        "encoded_reviews = [tokenizer.encode(sent, add_special_tokens=True) for sent in all_reviews]\n",
        "print(encoded_reviews[0])\n",
        "# Find the maximum length\n",
        "\n",
        "max_len = max([len(sent) for sent in encoded_reviews])\n",
        "print('Max length: ', max_len)\n",
        "#4m45s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCdogpyBRZZU",
        "outputId": "5d4bb5a0-80f9-4661-e441-cffa2b19745c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original:  One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.<br /><br />It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.<br /><br />I would say the main appeal of the show is due to the fact that it goes where other shows wouldn't dare. Forget pretty pictures painted for mainstream audiences, forget charm, forget romance...OZ doesn't mess around. The first episode I ever saw struck me as so nasty it was surreal, I couldn't say I was ready for it, but as I watched more, I developed a taste for Oz, and got accustomed to the high levels of graphic violence. Not just violence, but injustice (crooked guards who'll be sold out for a nickel, inmates who'll kill on order and get away with it, well mannered, middle class inmates being turned into prison bitches due to their lack of street skills or prison experience) Watching Oz, you may become comfortable with what is uncomfortable viewing....thats if you can get in touch with your darker side.\n",
            "Token IDs:  [101, 2028, 1997, 1996, 2060, 15814, 2038, 3855, 2008, 2044, 3666, 2074, 1015, 11472, 2792, 2017, 1005, 2222, 2022, 13322, 1012, 2027, 2024, 2157, 1010, 2004, 2023, 2003, 3599, 2054, 3047, 2007, 2033, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1996, 2034, 2518, 2008, 4930, 2033, 2055, 11472, 2001, 2049, 24083, 1998, 4895, 10258, 2378, 8450, 5019, 1997, 4808, 1010, 2029, 2275, 1999, 2157, 2013, 1996, 2773, 2175, 1012, 3404, 2033, 1010, 2023, 2003, 2025, 1037, 2265, 2005, 1996, 8143, 18627, 2030, 5199, 3593, 1012, 2023, 2265, 8005, 2053, 17957, 2007, 12362, 2000, 5850, 1010, 3348, 2030, 4808, 1012, 2049, 2003, 13076, 1010, 1999, 1996, 4438, 2224, 1997, 1996, 2773, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 2009, 2003, 2170, 11472, 2004, 2008, 2003, 1996, 8367, 2445, 2000, 1996, 17411, 4555, 3036, 2110, 7279, 4221, 12380, 2854, 1012, 2009, 7679, 3701, 2006, 14110, 2103, 1010, 2019, 6388, 2930, 1997, 1996, 3827, 2073, 2035, 1996, 4442, 2031, 3221, 21430, 1998, 2227, 20546, 2015, 1010, 2061, 9394, 2003, 2025, 2152, 2006, 1996, 11376, 1012, 7861, 2103, 2003, 2188, 2000, 2116, 1012, 1012, 26030, 2015, 1010, 7486, 1010, 18542, 10230, 1010, 7402, 2015, 1010, 8135, 1010, 16773, 1010, 3493, 1998, 2062, 1012, 1012, 1012, 1012, 2061, 8040, 16093, 28331, 1010, 2331, 14020, 1010, 26489, 6292, 24069, 1998, 22824, 10540, 2024, 2196, 2521, 2185, 1012, 1026, 7987, 1013, 1028, 1026, 7987, 1013, 1028, 1045, 2052, 2360, 1996, 2364, 5574, 1997, 1996, 2265, 2003, 2349, 2000, 1996, 2755, 2008, 2009, 3632, 2073, 2060, 3065, 2876, 1005, 1056, 8108, 1012, 5293, 3492, 4620, 4993, 2005, 7731, 9501, 1010, 5293, 11084, 1010, 5293, 7472, 1012, 1012, 1012, 11472, 2987, 1005, 1056, 6752, 2105, 1012, 1996, 2034, 2792, 1045, 2412, 2387, 4930, 2033, 2004, 2061, 11808, 2009, 2001, 16524, 1010, 1045, 2481, 1005, 1056, 2360, 1045, 2001, 3201, 2005, 2009, 1010, 2021, 2004, 1045, 3427, 2062, 1010, 1045, 2764, 1037, 5510, 2005, 11472, 1010, 1998, 2288, 17730, 2000, 1996, 2152, 3798, 1997, 8425, 4808, 1012, 2025, 2074, 4808, 1010, 2021, 21321, 1006, 15274, 4932, 2040, 1005, 2222, 2022, 2853, 2041, 2005, 1037, 15519, 1010, 13187, 2040, 1005, 2222, 3102, 2006, 2344, 1998, 2131, 2185, 2007, 2009, 1010, 2092, 5450, 2098, 1010, 2690, 2465, 13187, 2108, 2357, 2046, 3827, 7743, 2229, 2349, 2000, 2037, 3768, 1997, 2395, 4813, 2030, 3827, 3325, 1007, 3666, 11472, 1010, 2017, 2089, 2468, 6625, 2007, 2054, 2003, 8796, 10523, 1012, 1012, 1012, 1012, 2008, 2015, 2065, 2017, 2064, 2131, 1999, 3543, 2007, 2115, 9904, 2217, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Tokenizing data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2329: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        }
      ],
      "source": [
        "# Specify `MAX_LEN`\n",
        "MAX_LEN = 512\n",
        "\n",
        "# Print sentence 0 and its encoded token ids\n",
        "token_ids = list(preprocessing_for_bert([X[0]])[0].squeeze().numpy())\n",
        "print('Original: ', X[0])\n",
        "print('Token IDs: ', token_ids)\n",
        "\n",
        "# Run function `preprocessing_for_bert` on the train set and the validation set\n",
        "print('Tokenizing data...')\n",
        "train_inputs, train_masks = preprocessing_for_bert(X_train)\n",
        "val_inputs, val_masks = preprocessing_for_bert(X_val)\n",
        "#5m"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "meHJTw3LbKIa"
      },
      "source": [
        "***2.2. Create PyTorch DataLoader***\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "We will create an iterator for our dataset using the torch DataLoader class. This will help save on memory during training and boost the training speed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "723_J9hFbJ31"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# Convert other data types to torch.Tensor\n",
        "train_labels = torch.tensor(y_train)\n",
        "val_labels = torch.tensor(y_val)\n",
        "\n",
        "# For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\n",
        "batch_size = 4\n",
        "\n",
        "# Create the DataLoader for our training set\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our validation set\n",
        "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
        "val_sampler = SequentialSampler(val_data)\n",
        "val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0HGJ13-bbPK"
      },
      "source": [
        "## 3. Train Our Model ---\n",
        "\n",
        "***3.1. Create BertClassifier***\n",
        "---\n",
        "\n",
        "BERT-base consists of 12 transformer layers, each transformer layer takes in a list of token embeddings, and produces the same number of embeddings with the same hidden size (or dimensions) on the output. The output of the final transformer layer of the [CLS] token is used as the features of the sequence to feed a classifier.\n",
        "\n",
        "The transformers library has the BertForSequenceClassification class which is designed for classification tasks. However, we will create a new class so we can specify our own choice of classifiers.\n",
        "\n",
        "Below we will create a BertClassifier class with a BERT model to extract the last hidden layer of the [CLS] token and a single-hidden-layer feed-forward neural network as our classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTT_0haebnhF",
        "outputId": "3f57e552-644f-4ec5-aec1-8e7a7f339c01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 41 µs, sys: 0 ns, total: 41 µs\n",
            "Wall time: 50.8 µs\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertModel\n",
        "\n",
        "# Create the BertClassfier class\n",
        "class BertClassifier(nn.Module):\n",
        "    \"\"\"Bert Model for Classification Tasks.\n",
        "    \"\"\"\n",
        "    def __init__(self, freeze_bert=False):\n",
        "        \"\"\"\n",
        "        @param    bert: a BertModel object\n",
        "        @param    classifier: a torch.nn.Module classifier\n",
        "        @param    freeze_bert (bool): Set `False` to fine-tune the BERT model\n",
        "        \"\"\"\n",
        "        super(BertClassifier, self).__init__()\n",
        "        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n",
        "        D_in, H, D_out = 768, 50, 2\n",
        "\n",
        "        # Instantiate BERT model\n",
        "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "        # Instantiate an one-layer feed-forward classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(D_in, H),\n",
        "            nn.ReLU(),\n",
        "            #nn.Dropout(0.5),\n",
        "            nn.Linear(H, D_out)\n",
        "        )\n",
        "\n",
        "        # Freeze the BERT model\n",
        "        if freeze_bert:\n",
        "            for param in self.bert.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        \"\"\"\n",
        "        Feed input to BERT and the classifier to compute logits.\n",
        "        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,\n",
        "                      max_length)\n",
        "        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n",
        "                      information with shape (batch_size, max_length)\n",
        "        @return   logits (torch.Tensor): an output tensor with shape (batch_size,\n",
        "                      num_labels)\n",
        "        \"\"\"\n",
        "        # Feed input to BERT\n",
        "        outputs = self.bert(input_ids=input_ids,\n",
        "                            attention_mask=attention_mask)\n",
        "\n",
        "        # Extract the last hidden state of the token `[CLS]` for classification task\n",
        "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
        "\n",
        "        # Feed input to classifier to compute logits\n",
        "        logits = self.classifier(last_hidden_state_cls)\n",
        "\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jV0BxrVb0nh"
      },
      "source": [
        "***3.2. Optimizer & Learning Rate Scheduler***\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "To fine-tune our Bert Classifier, we need to create an optimizer. The authors recommend following hyper-parameters:\n",
        "\n",
        "Batch size: 16 or 32\n",
        "Learning rate (Adam): 5e-5, 3e-5 or 2e-5\n",
        "Number of epochs: 2, 3, 4\n",
        "Huggingface provided the run_glue.py script, an examples of implementing the transformers library. In the script, the AdamW optimizer is used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NRVFeijhbwPW"
      },
      "outputs": [],
      "source": [
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "def initialize_model(epochs=4):\n",
        "    \"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n",
        "    \"\"\"\n",
        "    # Instantiate Bert Classifier\n",
        "    bert_classifier = BertClassifier(freeze_bert=False)\n",
        "\n",
        "    # Tell PyTorch to run the model on GPU\n",
        "    bert_classifier.to(device)\n",
        "\n",
        "    # Create the optimizer\n",
        "    optimizer = AdamW(bert_classifier.parameters(),\n",
        "                      lr=5e-5,    # Default learning rate\n",
        "                      eps=1e-8    # Default epsilon value\n",
        "                      )\n",
        "\n",
        "    # Total number of training steps\n",
        "    total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "    # Set up the learning rate scheduler\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                                num_warmup_steps=0, # Default value\n",
        "                                                num_training_steps=total_steps)\n",
        "    return bert_classifier, optimizer, scheduler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OlAyHRhcLVR"
      },
      "source": [
        "### 3.3. Training Loop\n",
        "We will train our Bert Classifier for 4 epochs. In each epoch, we will train our model and evaluate its performance on the validation set. In more details, we will:\n",
        "\n",
        "***Training:***\n",
        "\n",
        "* Unpack our data from the dataloader and load the data onto the GPU\n",
        "* Zero out gradients calculated in the previous pass\n",
        "* Perform a forward pass to compute logits and loss\n",
        "* Perform a backward pass to compute gradients (loss.backward())\n",
        "* Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
        "* Update the model's parameters (optimizer.step())\n",
        "* Update the learning rate (scheduler.step())\n",
        "\n",
        "***Evaluation:***\n",
        "\n",
        "* Unpack our data and load onto the GPU\n",
        "* Forward pass\n",
        "* Compute loss and accuracy rate over the validation set\n",
        "\n",
        "The script below is commented with the details of our training and evaluation loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QFN51Te8b7v8"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import time\n",
        "\n",
        "# Specify loss function\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "def set_seed(seed_value=42):\n",
        "    \"\"\"Set seed for reproducibility.\n",
        "    \"\"\"\n",
        "    random.seed(seed_value)\n",
        "    np.random.seed(seed_value)\n",
        "    torch.manual_seed(seed_value)\n",
        "    torch.cuda.manual_seed_all(seed_value)\n",
        "\n",
        "def train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n",
        "    \"\"\"Train the BertClassifier model.\n",
        "    \"\"\"\n",
        "    # Start training loop\n",
        "    print(\"Start training...\\n\")\n",
        "    for epoch_i in range(epochs):\n",
        "        # =======================================\n",
        "        #               Training\n",
        "        # =======================================\n",
        "        # Print the header of the result table\n",
        "        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
        "        print(\"-\"*70)\n",
        "\n",
        "        # Measure the elapsed time of each epoch\n",
        "        t0_epoch, t0_batch = time.time(), time.time()\n",
        "\n",
        "        # Reset tracking variables at the beginning of each epoch\n",
        "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
        "\n",
        "        # Put the model into the training mode\n",
        "        model.train()\n",
        "\n",
        "        # For each batch of training data...\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            batch_counts +=1\n",
        "            # Load batch to GPU\n",
        "            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "            # Zero out any previously calculated gradients\n",
        "            model.zero_grad()\n",
        "\n",
        "            # Perform a forward pass. This will return logits.\n",
        "            logits = model(b_input_ids, b_attn_mask)\n",
        "\n",
        "            # Compute loss and accumulate the loss values\n",
        "            loss = loss_fn(logits, b_labels)\n",
        "            batch_loss += loss.item()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Perform a backward pass to calculate gradients\n",
        "            loss.backward()\n",
        "\n",
        "            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            # Update parameters and the learning rate\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            # Print the loss values and time elapsed for every 20 batches\n",
        "            if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
        "                # Calculate time elapsed for 20 batches\n",
        "                time_elapsed = time.time() - t0_batch\n",
        "\n",
        "                # Print training results\n",
        "                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
        "\n",
        "                # Reset batch tracking variables\n",
        "                batch_loss, batch_counts = 0, 0\n",
        "                t0_batch = time.time()\n",
        "\n",
        "        # Calculate the average loss over the entire training data\n",
        "        avg_train_loss = total_loss / len(train_dataloader)\n",
        "\n",
        "        print(\"-\"*70)\n",
        "        # =======================================\n",
        "        #               Evaluation\n",
        "        # =======================================\n",
        "        if evaluation == True:\n",
        "            # After the completion of each training epoch, measure the model's performance\n",
        "            # on our validation set.\n",
        "            val_loss, val_accuracy = evaluate(model, val_dataloader)\n",
        "\n",
        "            # Print performance over the entire training data\n",
        "            time_elapsed = time.time() - t0_epoch\n",
        "\n",
        "            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n",
        "            print(\"-\"*70)\n",
        "        print(\"\\n\")\n",
        "\n",
        "    print(\"Training complete!\")\n",
        "\n",
        "\n",
        "def evaluate(model, val_dataloader):\n",
        "    \"\"\"After the completion of each training epoch, measure the model's performance\n",
        "    on our validation set.\n",
        "    \"\"\"\n",
        "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
        "    # the test time.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables\n",
        "    val_accuracy = []\n",
        "    val_loss = []\n",
        "\n",
        "    # For each batch in our validation set...\n",
        "    for batch in val_dataloader:\n",
        "        # Load batch to GPU\n",
        "        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        # Compute logits\n",
        "        with torch.no_grad():\n",
        "            logits = model(b_input_ids, b_attn_mask)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = loss_fn(logits, b_labels)\n",
        "        val_loss.append(loss.item())\n",
        "\n",
        "        # Get the predictions\n",
        "        preds = torch.argmax(logits, dim=1).flatten()\n",
        "\n",
        "        # Calculate the accuracy rate\n",
        "        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
        "        val_accuracy.append(accuracy)\n",
        "\n",
        "    # Compute the average accuracy and loss over the validation set.\n",
        "    val_loss = np.mean(val_loss)\n",
        "    val_accuracy = np.mean(val_accuracy)\n",
        "\n",
        "    return val_loss, val_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "ac34aa20cdde4dc0bfc0acfcf12df439",
            "2df3de7381714f298d13d475b66c2d2f",
            "2d98c56ef1a242549ad95bfb1f57d6a4",
            "d2ced0cd2de944a2a6b252e9d644c787",
            "4255f39cb0fe40adbdd2d4b2a579bad2",
            "7384f9e2b7df4888a4c9d5a0113a68d1",
            "1823a63d42aa460a930b376f8aa5a720",
            "9cdf599b11b0495594325830e4b19973",
            "0e9126cdc9ff473481e4939956e2babd",
            "65f993a431c7417e883da7b70ec87eac",
            "d95e13793956456aae04e0f782d7566f"
          ]
        },
        "id": "ZP6thXBSckuO",
        "outputId": "d07d3565-3497-4a84-be75-21af3c82c131"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/420M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ac34aa20cdde4dc0bfc0acfcf12df439"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start training...\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   1    |   20    |   0.699251   |     -      |     -     |   10.34  \n",
            "   1    |   40    |   0.672845   |     -      |     -     |   7.35   \n",
            "   1    |   60    |   0.542739   |     -      |     -     |   7.44   \n",
            "   1    |   80    |   0.441602   |     -      |     -     |   7.48   \n",
            "   1    |   100   |   0.469836   |     -      |     -     |   7.56   \n",
            "   1    |   120   |   0.690046   |     -      |     -     |   7.61   \n",
            "   1    |   140   |   0.562597   |     -      |     -     |   7.65   \n",
            "   1    |   160   |   0.537557   |     -      |     -     |   7.76   \n",
            "   1    |   180   |   0.261514   |     -      |     -     |   7.76   \n",
            "   1    |   200   |   0.523439   |     -      |     -     |   7.87   \n",
            "   1    |   220   |   0.433443   |     -      |     -     |   7.88   \n",
            "   1    |   240   |   0.922521   |     -      |     -     |   8.01   \n",
            "   1    |   260   |   0.407089   |     -      |     -     |   8.02   \n",
            "   1    |   280   |   0.456440   |     -      |     -     |   8.07   \n",
            "   1    |   300   |   0.273453   |     -      |     -     |   8.06   \n",
            "   1    |   320   |   0.444209   |     -      |     -     |   8.03   \n",
            "   1    |   340   |   0.588786   |     -      |     -     |   8.01   \n",
            "   1    |   360   |   0.403868   |     -      |     -     |   7.93   \n",
            "   1    |   380   |   0.199020   |     -      |     -     |   7.96   \n",
            "   1    |   400   |   0.576390   |     -      |     -     |   7.93   \n",
            "   1    |   420   |   0.512130   |     -      |     -     |   7.96   \n",
            "   1    |   440   |   0.748292   |     -      |     -     |   7.97   \n",
            "   1    |   460   |   0.522483   |     -      |     -     |   8.01   \n",
            "   1    |   480   |   0.377771   |     -      |     -     |   7.96   \n",
            "   1    |   500   |   0.514942   |     -      |     -     |   8.01   \n",
            "   1    |   520   |   0.493638   |     -      |     -     |   8.04   \n",
            "   1    |   540   |   0.584149   |     -      |     -     |   7.96   \n",
            "   1    |   560   |   0.419064   |     -      |     -     |   8.01   \n",
            "   1    |   580   |   0.644831   |     -      |     -     |   8.02   \n",
            "   1    |   600   |   0.455915   |     -      |     -     |   8.00   \n",
            "   1    |   620   |   0.447161   |     -      |     -     |   7.99   \n",
            "   1    |   640   |   0.401101   |     -      |     -     |   7.94   \n",
            "   1    |   660   |   0.559685   |     -      |     -     |   7.99   \n",
            "   1    |   680   |   0.488793   |     -      |     -     |   7.97   \n",
            "   1    |   700   |   0.511410   |     -      |     -     |   7.96   \n",
            "   1    |   720   |   0.502239   |     -      |     -     |   7.98   \n",
            "   1    |   740   |   0.257267   |     -      |     -     |   7.99   \n",
            "   1    |   760   |   0.585609   |     -      |     -     |   8.04   \n",
            "   1    |   780   |   0.406142   |     -      |     -     |   7.99   \n",
            "   1    |   800   |   0.351461   |     -      |     -     |   7.98   \n",
            "   1    |   820   |   0.587196   |     -      |     -     |   7.98   \n",
            "   1    |   840   |   0.549144   |     -      |     -     |   7.99   \n",
            "   1    |   860   |   0.558147   |     -      |     -     |   7.97   \n",
            "   1    |   880   |   0.562556   |     -      |     -     |   7.96   \n",
            "   1    |   900   |   0.556205   |     -      |     -     |   7.98   \n",
            "   1    |   920   |   0.482612   |     -      |     -     |   7.99   \n",
            "   1    |   940   |   0.462622   |     -      |     -     |   7.97   \n",
            "   1    |   960   |   0.438844   |     -      |     -     |   8.01   \n",
            "   1    |   980   |   0.388018   |     -      |     -     |   7.97   \n",
            "   1    |  1000   |   0.486396   |     -      |     -     |   8.00   \n",
            "   1    |  1020   |   0.479514   |     -      |     -     |   8.03   \n",
            "   1    |  1040   |   0.430942   |     -      |     -     |   8.00   \n",
            "   1    |  1060   |   0.452886   |     -      |     -     |   8.01   \n",
            "   1    |  1080   |   0.432320   |     -      |     -     |   7.99   \n",
            "   1    |  1100   |   0.412348   |     -      |     -     |   7.99   \n",
            "   1    |  1120   |   0.621649   |     -      |     -     |   7.95   \n",
            "   1    |  1140   |   0.439369   |     -      |     -     |   7.97   \n",
            "   1    |  1160   |   0.448584   |     -      |     -     |   7.99   \n",
            "   1    |  1180   |   0.393091   |     -      |     -     |   8.01   \n",
            "   1    |  1200   |   0.282203   |     -      |     -     |   7.99   \n",
            "   1    |  1220   |   0.470547   |     -      |     -     |   7.99   \n",
            "   1    |  1240   |   0.532928   |     -      |     -     |   7.95   \n",
            "   1    |  1260   |   0.140638   |     -      |     -     |   7.97   \n",
            "   1    |  1280   |   0.590069   |     -      |     -     |   7.99   \n",
            "   1    |  1300   |   0.382091   |     -      |     -     |   8.00   \n",
            "   1    |  1320   |   0.368745   |     -      |     -     |   7.99   \n",
            "   1    |  1340   |   0.585801   |     -      |     -     |   7.98   \n",
            "   1    |  1360   |   0.354050   |     -      |     -     |   8.00   \n",
            "   1    |  1380   |   0.396224   |     -      |     -     |   7.99   \n",
            "   1    |  1400   |   0.315683   |     -      |     -     |   7.98   \n",
            "   1    |  1420   |   0.472872   |     -      |     -     |   7.98   \n",
            "   1    |  1440   |   0.674458   |     -      |     -     |   8.01   \n",
            "   1    |  1460   |   0.347370   |     -      |     -     |   7.99   \n",
            "   1    |  1480   |   0.414471   |     -      |     -     |   7.98   \n",
            "   1    |  1500   |   0.530016   |     -      |     -     |   8.01   \n",
            "   1    |  1520   |   0.377298   |     -      |     -     |   7.99   \n",
            "   1    |  1540   |   0.393996   |     -      |     -     |   8.02   \n",
            "   1    |  1560   |   0.500485   |     -      |     -     |   8.00   \n",
            "   1    |  1580   |   0.445123   |     -      |     -     |   8.00   \n",
            "   1    |  1600   |   0.264819   |     -      |     -     |   7.97   \n",
            "   1    |  1620   |   0.329620   |     -      |     -     |   7.99   \n",
            "   1    |  1640   |   0.659650   |     -      |     -     |   8.02   \n",
            "   1    |  1660   |   0.761762   |     -      |     -     |   8.01   \n",
            "   1    |  1680   |   0.263413   |     -      |     -     |   7.98   \n",
            "   1    |  1700   |   0.589834   |     -      |     -     |   7.99   \n",
            "   1    |  1720   |   0.518465   |     -      |     -     |   8.01   \n",
            "   1    |  1740   |   0.366755   |     -      |     -     |   7.99   \n",
            "   1    |  1760   |   0.491463   |     -      |     -     |   7.97   \n",
            "   1    |  1780   |   0.396008   |     -      |     -     |   7.98   \n",
            "   1    |  1800   |   0.349098   |     -      |     -     |   8.00   \n",
            "   1    |  1820   |   0.427126   |     -      |     -     |   7.94   \n",
            "   1    |  1840   |   0.441006   |     -      |     -     |   7.95   \n",
            "   1    |  1860   |   0.438170   |     -      |     -     |   8.01   \n",
            "   1    |  1880   |   0.169373   |     -      |     -     |   7.99   \n",
            "   1    |  1900   |   0.466164   |     -      |     -     |   8.02   \n",
            "   1    |  1920   |   0.516949   |     -      |     -     |   7.98   \n",
            "   1    |  1940   |   0.585718   |     -      |     -     |   7.95   \n",
            "   1    |  1960   |   0.423037   |     -      |     -     |   8.00   \n",
            "   1    |  1980   |   0.359346   |     -      |     -     |   7.98   \n",
            "   1    |  2000   |   0.453518   |     -      |     -     |   8.02   \n",
            "   1    |  2020   |   0.598712   |     -      |     -     |   7.96   \n",
            "   1    |  2040   |   0.332969   |     -      |     -     |   8.01   \n",
            "   1    |  2060   |   0.322567   |     -      |     -     |   8.00   \n",
            "   1    |  2080   |   0.259505   |     -      |     -     |   7.99   \n",
            "   1    |  2100   |   0.404141   |     -      |     -     |   7.98   \n",
            "   1    |  2120   |   0.345962   |     -      |     -     |   7.98   \n",
            "   1    |  2140   |   0.269564   |     -      |     -     |   8.01   \n",
            "   1    |  2160   |   0.353592   |     -      |     -     |   7.99   \n",
            "   1    |  2180   |   0.544954   |     -      |     -     |   8.01   \n",
            "   1    |  2200   |   0.460043   |     -      |     -     |   8.03   \n",
            "   1    |  2220   |   0.448757   |     -      |     -     |   8.01   \n",
            "   1    |  2240   |   0.281526   |     -      |     -     |   8.05   \n",
            "   1    |  2260   |   0.434872   |     -      |     -     |   7.96   \n",
            "   1    |  2280   |   0.447131   |     -      |     -     |   7.99   \n",
            "   1    |  2300   |   0.428715   |     -      |     -     |   7.95   \n",
            "   1    |  2320   |   0.220806   |     -      |     -     |   7.97   \n",
            "   1    |  2340   |   0.291434   |     -      |     -     |   7.98   \n",
            "   1    |  2360   |   0.618445   |     -      |     -     |   7.96   \n",
            "   1    |  2380   |   0.552429   |     -      |     -     |   7.97   \n",
            "   1    |  2400   |   0.471807   |     -      |     -     |   7.97   \n",
            "   1    |  2420   |   0.317595   |     -      |     -     |   7.96   \n",
            "   1    |  2440   |   0.532688   |     -      |     -     |   7.98   \n",
            "   1    |  2460   |   0.276662   |     -      |     -     |   7.96   \n",
            "   1    |  2480   |   0.419334   |     -      |     -     |   8.00   \n",
            "   1    |  2500   |   0.411910   |     -      |     -     |   7.99   \n",
            "   1    |  2520   |   0.325027   |     -      |     -     |   7.98   \n",
            "   1    |  2540   |   0.280797   |     -      |     -     |   8.02   \n",
            "   1    |  2560   |   0.226819   |     -      |     -     |   7.97   \n",
            "   1    |  2580   |   0.449353   |     -      |     -     |   7.99   \n",
            "   1    |  2600   |   0.263631   |     -      |     -     |   7.98   \n",
            "   1    |  2620   |   0.440346   |     -      |     -     |   8.00   \n",
            "   1    |  2640   |   0.428860   |     -      |     -     |   7.96   \n",
            "   1    |  2660   |   0.384713   |     -      |     -     |   7.97   \n",
            "   1    |  2680   |   0.447246   |     -      |     -     |   8.00   \n",
            "   1    |  2700   |   0.501556   |     -      |     -     |   8.00   \n",
            "   1    |  2720   |   0.338663   |     -      |     -     |   7.97   \n",
            "   1    |  2740   |   0.252384   |     -      |     -     |   7.98   \n",
            "   1    |  2760   |   0.132873   |     -      |     -     |   7.99   \n",
            "   1    |  2780   |   0.442828   |     -      |     -     |   7.99   \n",
            "   1    |  2800   |   0.222640   |     -      |     -     |   7.99   \n",
            "   1    |  2820   |   0.568325   |     -      |     -     |   7.99   \n",
            "   1    |  2840   |   0.313473   |     -      |     -     |   7.99   \n",
            "   1    |  2860   |   0.368093   |     -      |     -     |   7.98   \n",
            "   1    |  2880   |   0.159213   |     -      |     -     |   7.99   \n",
            "   1    |  2900   |   0.438434   |     -      |     -     |   8.02   \n",
            "   1    |  2920   |   0.301407   |     -      |     -     |   7.98   \n",
            "   1    |  2940   |   0.387782   |     -      |     -     |   8.01   \n",
            "   1    |  2960   |   0.301018   |     -      |     -     |   7.97   \n",
            "   1    |  2980   |   0.387400   |     -      |     -     |   7.98   \n",
            "   1    |  3000   |   0.214399   |     -      |     -     |   8.00   \n",
            "   1    |  3020   |   0.923607   |     -      |     -     |   7.99   \n",
            "   1    |  3040   |   0.295137   |     -      |     -     |   8.01   \n",
            "   1    |  3060   |   0.337140   |     -      |     -     |   7.99   \n",
            "   1    |  3080   |   0.242132   |     -      |     -     |   7.99   \n",
            "   1    |  3100   |   0.187157   |     -      |     -     |   8.01   \n",
            "   1    |  3120   |   0.621121   |     -      |     -     |   7.99   \n",
            "   1    |  3140   |   0.670758   |     -      |     -     |   8.00   \n",
            "   1    |  3160   |   0.506452   |     -      |     -     |   7.95   \n",
            "   1    |  3180   |   0.293539   |     -      |     -     |   8.00   \n",
            "   1    |  3200   |   0.408467   |     -      |     -     |   7.96   \n",
            "   1    |  3220   |   0.375434   |     -      |     -     |   7.99   \n",
            "   1    |  3240   |   0.443752   |     -      |     -     |   7.98   \n",
            "   1    |  3260   |   0.578541   |     -      |     -     |   7.96   \n",
            "   1    |  3280   |   0.321002   |     -      |     -     |   7.95   \n",
            "   1    |  3300   |   0.261864   |     -      |     -     |   7.99   \n",
            "   1    |  3320   |   0.472723   |     -      |     -     |   7.97   \n",
            "   1    |  3340   |   0.569688   |     -      |     -     |   7.97   \n",
            "   1    |  3360   |   0.311705   |     -      |     -     |   8.00   \n",
            "   1    |  3380   |   0.559348   |     -      |     -     |   7.99   \n",
            "   1    |  3400   |   0.401944   |     -      |     -     |   7.99   \n",
            "   1    |  3420   |   0.316005   |     -      |     -     |   7.95   \n",
            "   1    |  3440   |   0.541737   |     -      |     -     |   8.00   \n",
            "   1    |  3460   |   0.326120   |     -      |     -     |   7.97   \n",
            "   1    |  3480   |   0.321287   |     -      |     -     |   7.97   \n",
            "   1    |  3500   |   0.334890   |     -      |     -     |   7.95   \n",
            "   1    |  3520   |   0.538083   |     -      |     -     |   8.02   \n",
            "   1    |  3540   |   0.533569   |     -      |     -     |   7.97   \n",
            "   1    |  3560   |   0.370070   |     -      |     -     |   7.97   \n",
            "   1    |  3580   |   0.545211   |     -      |     -     |   8.01   \n",
            "   1    |  3600   |   0.556907   |     -      |     -     |   7.99   \n",
            "   1    |  3620   |   0.203778   |     -      |     -     |   7.98   \n",
            "   1    |  3640   |   0.274509   |     -      |     -     |   7.99   \n",
            "   1    |  3660   |   0.637551   |     -      |     -     |   8.01   \n",
            "   1    |  3680   |   0.431293   |     -      |     -     |   8.02   \n",
            "   1    |  3700   |   0.091135   |     -      |     -     |   7.98   \n",
            "   1    |  3720   |   0.510539   |     -      |     -     |   8.02   \n",
            "   1    |  3740   |   0.635905   |     -      |     -     |   7.99   \n",
            "   1    |  3760   |   0.290091   |     -      |     -     |   7.97   \n",
            "   1    |  3780   |   0.382528   |     -      |     -     |   7.99   \n",
            "   1    |  3800   |   0.796515   |     -      |     -     |   7.96   \n",
            "   1    |  3820   |   0.252317   |     -      |     -     |   7.98   \n",
            "   1    |  3840   |   0.590758   |     -      |     -     |   7.99   \n",
            "   1    |  3860   |   0.433114   |     -      |     -     |   8.01   \n",
            "   1    |  3880   |   0.421400   |     -      |     -     |   7.99   \n",
            "   1    |  3900   |   0.357352   |     -      |     -     |   7.99   \n",
            "   1    |  3920   |   0.563107   |     -      |     -     |   8.04   \n",
            "   1    |  3940   |   0.395068   |     -      |     -     |   7.96   \n",
            "   1    |  3960   |   0.337361   |     -      |     -     |   8.01   \n",
            "   1    |  3980   |   0.400451   |     -      |     -     |   7.98   \n",
            "   1    |  4000   |   0.265531   |     -      |     -     |   7.96   \n",
            "   1    |  4020   |   0.507783   |     -      |     -     |   7.95   \n",
            "   1    |  4040   |   0.415370   |     -      |     -     |   7.99   \n",
            "   1    |  4060   |   0.324772   |     -      |     -     |   7.98   \n",
            "   1    |  4080   |   0.431576   |     -      |     -     |   8.00   \n",
            "   1    |  4100   |   0.207135   |     -      |     -     |   8.00   \n",
            "   1    |  4120   |   0.404320   |     -      |     -     |   8.01   \n",
            "   1    |  4140   |   0.308966   |     -      |     -     |   7.96   \n",
            "   1    |  4160   |   0.358348   |     -      |     -     |   7.97   \n",
            "   1    |  4180   |   0.134222   |     -      |     -     |   7.98   \n",
            "   1    |  4200   |   0.399962   |     -      |     -     |   8.01   \n",
            "   1    |  4220   |   0.354060   |     -      |     -     |   7.96   \n",
            "   1    |  4240   |   0.812749   |     -      |     -     |   8.02   \n",
            "   1    |  4260   |   0.369857   |     -      |     -     |   7.99   \n",
            "   1    |  4280   |   0.138985   |     -      |     -     |   7.94   \n",
            "   1    |  4300   |   0.506902   |     -      |     -     |   8.00   \n",
            "   1    |  4320   |   0.241704   |     -      |     -     |   7.97   \n",
            "   1    |  4340   |   0.565431   |     -      |     -     |   7.97   \n",
            "   1    |  4360   |   0.375408   |     -      |     -     |   8.00   \n",
            "   1    |  4380   |   0.387077   |     -      |     -     |   8.00   \n",
            "   1    |  4400   |   0.424125   |     -      |     -     |   7.99   \n",
            "   1    |  4420   |   0.407041   |     -      |     -     |   7.99   \n",
            "   1    |  4440   |   0.323888   |     -      |     -     |   7.97   \n",
            "   1    |  4460   |   0.469447   |     -      |     -     |   7.99   \n",
            "   1    |  4480   |   0.676020   |     -      |     -     |   7.99   \n",
            "   1    |  4500   |   0.203847   |     -      |     -     |   7.97   \n",
            "   1    |  4520   |   0.504687   |     -      |     -     |   7.96   \n",
            "   1    |  4540   |   0.376937   |     -      |     -     |   8.01   \n",
            "   1    |  4560   |   0.214070   |     -      |     -     |   8.01   \n",
            "   1    |  4580   |   0.363205   |     -      |     -     |   7.98   \n",
            "   1    |  4600   |   0.385903   |     -      |     -     |   7.99   \n",
            "   1    |  4620   |   0.401258   |     -      |     -     |   8.04   \n",
            "   1    |  4640   |   0.389409   |     -      |     -     |   8.02   \n",
            "   1    |  4660   |   0.245210   |     -      |     -     |   8.02   \n",
            "   1    |  4680   |   0.550986   |     -      |     -     |   7.98   \n",
            "   1    |  4700   |   0.459368   |     -      |     -     |   8.00   \n",
            "   1    |  4720   |   0.325502   |     -      |     -     |   8.02   \n",
            "   1    |  4740   |   0.284316   |     -      |     -     |   7.98   \n",
            "   1    |  4760   |   0.783841   |     -      |     -     |   8.00   \n",
            "   1    |  4780   |   0.577818   |     -      |     -     |   7.94   \n",
            "   1    |  4800   |   0.299347   |     -      |     -     |   7.97   \n",
            "   1    |  4820   |   0.327567   |     -      |     -     |   7.97   \n",
            "   1    |  4840   |   0.507758   |     -      |     -     |   8.01   \n",
            "   1    |  4860   |   0.357491   |     -      |     -     |   7.99   \n",
            "   1    |  4880   |   0.230324   |     -      |     -     |   7.99   \n",
            "   1    |  4900   |   0.435404   |     -      |     -     |   8.03   \n",
            "   1    |  4920   |   0.325575   |     -      |     -     |   7.99   \n",
            "   1    |  4940   |   0.603174   |     -      |     -     |   7.99   \n",
            "   1    |  4960   |   0.326906   |     -      |     -     |   7.98   \n",
            "   1    |  4980   |   0.775937   |     -      |     -     |   8.02   \n",
            "   1    |  5000   |   0.197100   |     -      |     -     |   7.97   \n",
            "   1    |  5020   |   0.374068   |     -      |     -     |   7.97   \n",
            "   1    |  5040   |   0.212526   |     -      |     -     |   7.95   \n",
            "   1    |  5060   |   0.535700   |     -      |     -     |   8.00   \n",
            "   1    |  5080   |   0.409659   |     -      |     -     |   7.99   \n",
            "   1    |  5100   |   0.364556   |     -      |     -     |   7.99   \n",
            "   1    |  5120   |   0.671299   |     -      |     -     |   7.94   \n",
            "   1    |  5140   |   0.437311   |     -      |     -     |   8.04   \n",
            "   1    |  5160   |   0.390848   |     -      |     -     |   7.96   \n",
            "   1    |  5180   |   0.281882   |     -      |     -     |   7.97   \n",
            "   1    |  5200   |   0.370538   |     -      |     -     |   8.02   \n",
            "   1    |  5220   |   0.373039   |     -      |     -     |   7.97   \n",
            "   1    |  5240   |   0.403544   |     -      |     -     |   8.04   \n",
            "   1    |  5260   |   0.366299   |     -      |     -     |   7.96   \n",
            "   1    |  5280   |   0.368460   |     -      |     -     |   7.99   \n",
            "   1    |  5300   |   0.542082   |     -      |     -     |   7.97   \n",
            "   1    |  5320   |   0.192488   |     -      |     -     |   8.00   \n",
            "   1    |  5340   |   0.253651   |     -      |     -     |   7.99   \n",
            "   1    |  5360   |   0.425017   |     -      |     -     |   7.96   \n",
            "   1    |  5380   |   0.329040   |     -      |     -     |   7.99   \n",
            "   1    |  5400   |   0.271903   |     -      |     -     |   8.01   \n",
            "   1    |  5420   |   0.315539   |     -      |     -     |   8.01   \n",
            "   1    |  5440   |   0.289093   |     -      |     -     |   8.01   \n",
            "   1    |  5460   |   0.654138   |     -      |     -     |   8.00   \n",
            "   1    |  5480   |   0.291554   |     -      |     -     |   7.99   \n",
            "   1    |  5500   |   0.320165   |     -      |     -     |   8.01   \n",
            "   1    |  5520   |   0.547128   |     -      |     -     |   8.01   \n",
            "   1    |  5540   |   0.585881   |     -      |     -     |   8.00   \n",
            "   1    |  5560   |   0.321711   |     -      |     -     |   7.98   \n",
            "   1    |  5580   |   0.193012   |     -      |     -     |   7.95   \n",
            "   1    |  5600   |   0.705899   |     -      |     -     |   7.98   \n",
            "   1    |  5620   |   0.348311   |     -      |     -     |   7.97   \n",
            "   1    |  5640   |   0.220672   |     -      |     -     |   8.00   \n",
            "   1    |  5660   |   0.392746   |     -      |     -     |   7.97   \n",
            "   1    |  5680   |   0.334303   |     -      |     -     |   8.02   \n",
            "   1    |  5700   |   0.344630   |     -      |     -     |   7.98   \n",
            "   1    |  5720   |   0.388723   |     -      |     -     |   7.96   \n",
            "   1    |  5740   |   0.385116   |     -      |     -     |   7.97   \n",
            "   1    |  5760   |   0.306150   |     -      |     -     |   7.95   \n",
            "   1    |  5780   |   0.419371   |     -      |     -     |   7.98   \n",
            "   1    |  5800   |   0.319496   |     -      |     -     |   7.95   \n",
            "   1    |  5820   |   0.338868   |     -      |     -     |   7.97   \n",
            "   1    |  5840   |   0.390710   |     -      |     -     |   7.97   \n",
            "   1    |  5860   |   0.485019   |     -      |     -     |   8.00   \n",
            "   1    |  5880   |   0.307114   |     -      |     -     |   8.01   \n",
            "   1    |  5900   |   0.134379   |     -      |     -     |   7.99   \n",
            "   1    |  5920   |   0.343983   |     -      |     -     |   7.95   \n",
            "   1    |  5940   |   0.436171   |     -      |     -     |   8.00   \n",
            "   1    |  5960   |   0.225174   |     -      |     -     |   8.00   \n",
            "   1    |  5980   |   0.281190   |     -      |     -     |   8.00   \n",
            "   1    |  6000   |   0.325853   |     -      |     -     |   7.99   \n",
            "   1    |  6020   |   0.527252   |     -      |     -     |   8.00   \n",
            "   1    |  6040   |   0.495620   |     -      |     -     |   7.98   \n",
            "   1    |  6060   |   0.242351   |     -      |     -     |   7.98   \n",
            "   1    |  6080   |   0.177871   |     -      |     -     |   7.96   \n",
            "   1    |  6100   |   0.520042   |     -      |     -     |   8.01   \n",
            "   1    |  6120   |   0.487970   |     -      |     -     |   7.99   \n",
            "   1    |  6140   |   0.511581   |     -      |     -     |   7.96   \n",
            "   1    |  6160   |   0.301754   |     -      |     -     |   7.95   \n",
            "   1    |  6180   |   0.436867   |     -      |     -     |   8.02   \n",
            "   1    |  6200   |   0.449727   |     -      |     -     |   7.98   \n",
            "   1    |  6220   |   0.362453   |     -      |     -     |   8.00   \n",
            "   1    |  6240   |   0.112423   |     -      |     -     |   7.97   \n",
            "   1    |  6260   |   0.564331   |     -      |     -     |   8.01   \n",
            "   1    |  6280   |   0.548344   |     -      |     -     |   7.99   \n",
            "   1    |  6300   |   0.375603   |     -      |     -     |   7.97   \n",
            "   1    |  6320   |   0.568054   |     -      |     -     |   8.02   \n",
            "   1    |  6340   |   0.427323   |     -      |     -     |   7.95   \n",
            "   1    |  6360   |   0.419629   |     -      |     -     |   8.00   \n",
            "   1    |  6380   |   0.378898   |     -      |     -     |   7.97   \n",
            "   1    |  6400   |   0.421173   |     -      |     -     |   7.97   \n",
            "   1    |  6420   |   0.268719   |     -      |     -     |   7.96   \n",
            "   1    |  6440   |   0.363388   |     -      |     -     |   7.99   \n",
            "   1    |  6460   |   0.312099   |     -      |     -     |   7.98   \n",
            "   1    |  6480   |   0.286022   |     -      |     -     |   8.00   \n",
            "   1    |  6500   |   0.451415   |     -      |     -     |   7.97   \n",
            "   1    |  6520   |   0.461662   |     -      |     -     |   7.97   \n",
            "   1    |  6540   |   0.469000   |     -      |     -     |   7.97   \n",
            "   1    |  6560   |   0.425253   |     -      |     -     |   7.99   \n",
            "   1    |  6580   |   0.350105   |     -      |     -     |   7.98   \n",
            "   1    |  6600   |   0.214609   |     -      |     -     |   7.95   \n",
            "   1    |  6620   |   0.573039   |     -      |     -     |   7.96   \n",
            "   1    |  6640   |   0.358238   |     -      |     -     |   7.99   \n",
            "   1    |  6660   |   0.293490   |     -      |     -     |   8.00   \n",
            "   1    |  6680   |   0.598355   |     -      |     -     |   8.02   \n",
            "   1    |  6700   |   0.290169   |     -      |     -     |   7.98   \n",
            "   1    |  6720   |   0.570095   |     -      |     -     |   7.99   \n",
            "   1    |  6740   |   0.355219   |     -      |     -     |   8.02   \n",
            "   1    |  6760   |   0.344039   |     -      |     -     |   7.96   \n",
            "   1    |  6780   |   0.662962   |     -      |     -     |   8.01   \n",
            "   1    |  6800   |   0.338103   |     -      |     -     |   7.98   \n",
            "   1    |  6820   |   0.329677   |     -      |     -     |   7.97   \n",
            "   1    |  6840   |   0.410088   |     -      |     -     |   8.00   \n",
            "   1    |  6860   |   0.326866   |     -      |     -     |   7.98   \n",
            "   1    |  6880   |   0.244991   |     -      |     -     |   8.01   \n",
            "   1    |  6900   |   0.242118   |     -      |     -     |   7.98   \n",
            "   1    |  6920   |   0.497422   |     -      |     -     |   8.02   \n",
            "   1    |  6940   |   0.453968   |     -      |     -     |   8.01   \n",
            "   1    |  6960   |   0.248938   |     -      |     -     |   7.99   \n",
            "   1    |  6980   |   0.380742   |     -      |     -     |   7.98   \n",
            "   1    |  7000   |   0.230143   |     -      |     -     |   7.99   \n",
            "   1    |  7020   |   0.319798   |     -      |     -     |   7.97   \n",
            "   1    |  7040   |   0.441079   |     -      |     -     |   8.00   \n",
            "   1    |  7060   |   0.235503   |     -      |     -     |   8.01   \n",
            "   1    |  7080   |   0.314156   |     -      |     -     |   8.00   \n",
            "   1    |  7100   |   0.768317   |     -      |     -     |   8.01   \n",
            "   1    |  7120   |   0.430326   |     -      |     -     |   7.99   \n",
            "   1    |  7140   |   0.461323   |     -      |     -     |   8.00   \n",
            "   1    |  7160   |   0.437977   |     -      |     -     |   8.00   \n",
            "   1    |  7180   |   0.217504   |     -      |     -     |   7.97   \n",
            "   1    |  7200   |   0.301407   |     -      |     -     |   7.96   \n",
            "   1    |  7220   |   0.343108   |     -      |     -     |   7.99   \n",
            "   1    |  7240   |   0.291164   |     -      |     -     |   7.99   \n",
            "   1    |  7260   |   0.258351   |     -      |     -     |   7.96   \n",
            "   1    |  7280   |   0.300846   |     -      |     -     |   7.98   \n",
            "   1    |  7300   |   0.459103   |     -      |     -     |   8.02   \n",
            "   1    |  7320   |   0.565627   |     -      |     -     |   7.98   \n",
            "   1    |  7340   |   0.383952   |     -      |     -     |   7.99   \n",
            "   1    |  7360   |   0.284212   |     -      |     -     |   7.98   \n",
            "   1    |  7380   |   0.284068   |     -      |     -     |   7.98   \n",
            "   1    |  7400   |   0.460971   |     -      |     -     |   7.97   \n",
            "   1    |  7420   |   0.392511   |     -      |     -     |   7.97   \n",
            "   1    |  7440   |   0.479029   |     -      |     -     |   7.98   \n",
            "   1    |  7460   |   0.405399   |     -      |     -     |   7.97   \n",
            "   1    |  7480   |   0.400121   |     -      |     -     |   7.97   \n",
            "   1    |  7500   |   0.270858   |     -      |     -     |   7.94   \n",
            "   1    |  7520   |   0.133197   |     -      |     -     |   7.98   \n",
            "   1    |  7540   |   0.474805   |     -      |     -     |   8.00   \n",
            "   1    |  7560   |   0.423032   |     -      |     -     |   7.98   \n",
            "   1    |  7580   |   0.460066   |     -      |     -     |   7.97   \n",
            "   1    |  7600   |   0.481656   |     -      |     -     |   7.97   \n",
            "   1    |  7620   |   0.326447   |     -      |     -     |   7.94   \n",
            "   1    |  7640   |   0.362224   |     -      |     -     |   8.00   \n",
            "   1    |  7660   |   0.387018   |     -      |     -     |   7.95   \n",
            "   1    |  7680   |   0.363888   |     -      |     -     |   7.97   \n",
            "   1    |  7700   |   0.625568   |     -      |     -     |   8.00   \n",
            "   1    |  7720   |   0.227330   |     -      |     -     |   7.97   \n",
            "   1    |  7740   |   0.348030   |     -      |     -     |   8.03   \n",
            "   1    |  7760   |   0.689087   |     -      |     -     |   8.03   \n",
            "   1    |  7780   |   0.235695   |     -      |     -     |   7.99   \n",
            "   1    |  7800   |   0.321666   |     -      |     -     |   7.98   \n",
            "   1    |  7820   |   0.375273   |     -      |     -     |   7.96   \n",
            "   1    |  7840   |   0.196086   |     -      |     -     |   7.99   \n",
            "   1    |  7860   |   0.456612   |     -      |     -     |   7.94   \n",
            "   1    |  7880   |   0.341235   |     -      |     -     |   7.96   \n",
            "   1    |  7900   |   0.391886   |     -      |     -     |   7.97   \n",
            "   1    |  7920   |   0.443590   |     -      |     -     |   7.97   \n",
            "   1    |  7940   |   0.584219   |     -      |     -     |   7.97   \n",
            "   1    |  7960   |   0.252716   |     -      |     -     |   7.94   \n",
            "   1    |  7980   |   0.562723   |     -      |     -     |   7.97   \n",
            "   1    |  8000   |   0.190166   |     -      |     -     |   7.98   \n",
            "   1    |  8020   |   0.246134   |     -      |     -     |   7.96   \n",
            "   1    |  8040   |   0.534031   |     -      |     -     |   7.95   \n",
            "   1    |  8060   |   0.326161   |     -      |     -     |   8.01   \n",
            "   1    |  8080   |   0.343024   |     -      |     -     |   7.99   \n",
            "   1    |  8100   |   0.206922   |     -      |     -     |   7.96   \n",
            "   1    |  8120   |   0.524607   |     -      |     -     |   7.99   \n",
            "   1    |  8140   |   0.385489   |     -      |     -     |   8.01   \n",
            "   1    |  8160   |   0.286546   |     -      |     -     |   8.00   \n",
            "   1    |  8180   |   0.192387   |     -      |     -     |   7.97   \n",
            "   1    |  8200   |   0.334764   |     -      |     -     |   7.99   \n",
            "   1    |  8220   |   0.305651   |     -      |     -     |   8.00   \n",
            "   1    |  8240   |   0.412321   |     -      |     -     |   7.99   \n",
            "   1    |  8260   |   0.251605   |     -      |     -     |   7.93   \n",
            "   1    |  8280   |   0.380291   |     -      |     -     |   7.98   \n",
            "   1    |  8300   |   0.266168   |     -      |     -     |   7.95   \n",
            "   1    |  8320   |   0.676709   |     -      |     -     |   8.01   \n",
            "   1    |  8340   |   0.359519   |     -      |     -     |   7.96   \n",
            "   1    |  8360   |   0.475532   |     -      |     -     |   7.94   \n",
            "   1    |  8380   |   0.379585   |     -      |     -     |   8.02   \n",
            "   1    |  8400   |   0.215578   |     -      |     -     |   7.97   \n",
            "   1    |  8420   |   0.245202   |     -      |     -     |   8.00   \n",
            "   1    |  8440   |   0.228390   |     -      |     -     |   7.95   \n",
            "   1    |  8460   |   0.604910   |     -      |     -     |   7.98   \n",
            "   1    |  8480   |   0.308443   |     -      |     -     |   7.98   \n",
            "   1    |  8500   |   0.254143   |     -      |     -     |   7.98   \n",
            "   1    |  8520   |   0.212802   |     -      |     -     |   7.93   \n",
            "   1    |  8540   |   0.276364   |     -      |     -     |   8.03   \n",
            "   1    |  8560   |   0.427756   |     -      |     -     |   7.96   \n",
            "   1    |  8580   |   0.289865   |     -      |     -     |   7.96   \n",
            "   1    |  8600   |   0.357435   |     -      |     -     |   7.98   \n",
            "   1    |  8620   |   0.377741   |     -      |     -     |   7.95   \n",
            "   1    |  8640   |   0.220095   |     -      |     -     |   7.98   \n",
            "   1    |  8660   |   0.554785   |     -      |     -     |   7.98   \n",
            "   1    |  8680   |   0.525216   |     -      |     -     |   7.98   \n",
            "   1    |  8700   |   0.191177   |     -      |     -     |   7.99   \n",
            "   1    |  8720   |   0.359615   |     -      |     -     |   7.98   \n",
            "   1    |  8740   |   0.465827   |     -      |     -     |   7.97   \n",
            "   1    |  8760   |   0.306452   |     -      |     -     |   7.97   \n",
            "   1    |  8780   |   0.519986   |     -      |     -     |   7.97   \n",
            "   1    |  8800   |   0.583529   |     -      |     -     |   7.97   \n",
            "   1    |  8820   |   0.746928   |     -      |     -     |   8.01   \n",
            "   1    |  8840   |   0.219655   |     -      |     -     |   7.98   \n",
            "   1    |  8860   |   0.282309   |     -      |     -     |   7.99   \n",
            "   1    |  8880   |   0.339686   |     -      |     -     |   7.99   \n",
            "   1    |  8900   |   0.633022   |     -      |     -     |   7.98   \n",
            "   1    |  8920   |   0.271775   |     -      |     -     |   7.98   \n",
            "   1    |  8940   |   0.316586   |     -      |     -     |   7.98   \n",
            "   1    |  8960   |   0.215136   |     -      |     -     |   7.99   \n",
            "   1    |  8980   |   0.389491   |     -      |     -     |   7.97   \n",
            "   1    |  9000   |   0.341280   |     -      |     -     |   7.97   \n",
            "   1    |  9020   |   0.345476   |     -      |     -     |   7.99   \n",
            "   1    |  9040   |   0.280466   |     -      |     -     |   7.98   \n",
            "   1    |  9060   |   0.517060   |     -      |     -     |   7.98   \n",
            "   1    |  9080   |   0.124928   |     -      |     -     |   7.97   \n",
            "   1    |  9100   |   0.374186   |     -      |     -     |   7.98   \n",
            "   1    |  9120   |   0.219531   |     -      |     -     |   7.98   \n",
            "   1    |  9140   |   0.225253   |     -      |     -     |   7.97   \n",
            "   1    |  9160   |   0.544097   |     -      |     -     |   7.98   \n",
            "   1    |  9180   |   0.395929   |     -      |     -     |   7.96   \n",
            "   1    |  9200   |   0.290864   |     -      |     -     |   7.97   \n",
            "   1    |  9220   |   0.106062   |     -      |     -     |   7.96   \n",
            "   1    |  9240   |   0.497673   |     -      |     -     |   7.94   \n",
            "   1    |  9260   |   0.358243   |     -      |     -     |   7.99   \n",
            "   1    |  9280   |   0.313915   |     -      |     -     |   7.95   \n",
            "   1    |  9300   |   0.382298   |     -      |     -     |   7.95   \n",
            "   1    |  9320   |   0.231912   |     -      |     -     |   7.99   \n",
            "   1    |  9340   |   0.354359   |     -      |     -     |   8.00   \n",
            "   1    |  9360   |   0.612894   |     -      |     -     |   7.99   \n",
            "   1    |  9380   |   0.492206   |     -      |     -     |   7.97   \n",
            "   1    |  9400   |   0.581303   |     -      |     -     |   7.97   \n",
            "   1    |  9420   |   0.446478   |     -      |     -     |   7.96   \n",
            "   1    |  9440   |   0.274713   |     -      |     -     |   7.95   \n",
            "   1    |  9460   |   0.358739   |     -      |     -     |   7.96   \n",
            "   1    |  9480   |   0.134465   |     -      |     -     |   7.94   \n",
            "   1    |  9500   |   0.600441   |     -      |     -     |   7.98   \n",
            "   1    |  9520   |   0.244622   |     -      |     -     |   7.97   \n",
            "   1    |  9540   |   0.382448   |     -      |     -     |   7.97   \n",
            "   1    |  9560   |   0.316667   |     -      |     -     |   7.98   \n",
            "   1    |  9580   |   0.483720   |     -      |     -     |   7.99   \n",
            "   1    |  9600   |   0.555206   |     -      |     -     |   8.00   \n",
            "   1    |  9620   |   0.304877   |     -      |     -     |   7.99   \n",
            "   1    |  9640   |   0.193663   |     -      |     -     |   7.95   \n",
            "   1    |  9660   |   0.286286   |     -      |     -     |   7.97   \n",
            "   1    |  9680   |   0.437636   |     -      |     -     |   7.97   \n",
            "   1    |  9700   |   0.220316   |     -      |     -     |   8.01   \n",
            "   1    |  9720   |   0.153145   |     -      |     -     |   7.98   \n",
            "   1    |  9740   |   0.284657   |     -      |     -     |   7.97   \n",
            "   1    |  9760   |   0.287614   |     -      |     -     |   7.96   \n",
            "   1    |  9780   |   0.290803   |     -      |     -     |   7.99   \n",
            "   1    |  9800   |   0.439991   |     -      |     -     |   8.00   \n",
            "   1    |  9820   |   0.197157   |     -      |     -     |   7.98   \n",
            "   1    |  9840   |   0.411742   |     -      |     -     |   8.01   \n",
            "   1    |  9860   |   0.335431   |     -      |     -     |   7.98   \n",
            "   1    |  9880   |   0.243102   |     -      |     -     |   7.99   \n",
            "   1    |  9900   |   0.691035   |     -      |     -     |   8.01   \n",
            "   1    |  9920   |   0.709242   |     -      |     -     |   7.97   \n",
            "   1    |  9940   |   0.349907   |     -      |     -     |   8.00   \n",
            "   1    |  9960   |   0.138956   |     -      |     -     |   7.96   \n",
            "   1    |  9980   |   0.524587   |     -      |     -     |   7.96   \n",
            "   1    |  9999   |   0.144851   |     -      |     -     |   7.58   \n",
            "----------------------------------------------------------------------\n",
            "   1    |    -    |   0.400463   |  0.344427  |   92.31   |  4319.39 \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
            "----------------------------------------------------------------------\n",
            "   2    |   20    |   0.277731   |     -      |     -     |   8.39   \n",
            "   2    |   40    |   0.123468   |     -      |     -     |   7.99   \n",
            "   2    |   60    |   0.005090   |     -      |     -     |   7.99   \n",
            "   2    |   80    |   0.223287   |     -      |     -     |   7.96   \n",
            "   2    |   100   |   0.147178   |     -      |     -     |   7.98   \n",
            "   2    |   120   |   0.217143   |     -      |     -     |   7.98   \n",
            "   2    |   140   |   0.145953   |     -      |     -     |   7.97   \n",
            "   2    |   160   |   0.265752   |     -      |     -     |   7.98   \n",
            "   2    |   180   |   0.424586   |     -      |     -     |   7.97   \n",
            "   2    |   200   |   0.225152   |     -      |     -     |   7.97   \n",
            "   2    |   220   |   0.205105   |     -      |     -     |   7.99   \n",
            "   2    |   240   |   0.246858   |     -      |     -     |   7.96   \n",
            "   2    |   260   |   0.158077   |     -      |     -     |   7.97   \n",
            "   2    |   280   |   0.212740   |     -      |     -     |   7.98   \n",
            "   2    |   300   |   0.237899   |     -      |     -     |   7.99   \n",
            "   2    |   320   |   0.005013   |     -      |     -     |   7.98   \n",
            "   2    |   340   |   0.191387   |     -      |     -     |   7.97   \n",
            "   2    |   360   |   0.240363   |     -      |     -     |   7.98   \n",
            "   2    |   380   |   0.274320   |     -      |     -     |   7.93   \n",
            "   2    |   400   |   0.155521   |     -      |     -     |   7.96   \n",
            "   2    |   420   |   0.271249   |     -      |     -     |   7.96   \n",
            "   2    |   440   |   0.138867   |     -      |     -     |   7.97   \n",
            "   2    |   460   |   0.077674   |     -      |     -     |   7.94   \n",
            "   2    |   480   |   0.367847   |     -      |     -     |   8.02   \n",
            "   2    |   500   |   0.217170   |     -      |     -     |   7.97   \n",
            "   2    |   520   |   0.202482   |     -      |     -     |   8.00   \n",
            "   2    |   540   |   0.257337   |     -      |     -     |   7.98   \n",
            "   2    |   560   |   0.071203   |     -      |     -     |   7.96   \n",
            "   2    |   580   |   0.273130   |     -      |     -     |   7.95   \n",
            "   2    |   600   |   0.128054   |     -      |     -     |   7.93   \n",
            "   2    |   620   |   0.261548   |     -      |     -     |   7.96   \n",
            "   2    |   640   |   0.062256   |     -      |     -     |   7.98   \n",
            "   2    |   660   |   0.124594   |     -      |     -     |   7.95   \n",
            "   2    |   680   |   0.087466   |     -      |     -     |   7.98   \n",
            "   2    |   700   |   0.134188   |     -      |     -     |   8.00   \n",
            "   2    |   720   |   0.171174   |     -      |     -     |   8.00   \n",
            "   2    |   740   |   0.054269   |     -      |     -     |   7.99   \n",
            "   2    |   760   |   0.541064   |     -      |     -     |   7.96   \n",
            "   2    |   780   |   0.070458   |     -      |     -     |   7.99   \n",
            "   2    |   800   |   0.204317   |     -      |     -     |   7.95   \n",
            "   2    |   820   |   0.418975   |     -      |     -     |   7.95   \n",
            "   2    |   840   |   0.351613   |     -      |     -     |   7.95   \n",
            "   2    |   860   |   0.309795   |     -      |     -     |   7.99   \n",
            "   2    |   880   |   0.218018   |     -      |     -     |   7.96   \n",
            "   2    |   900   |   0.080454   |     -      |     -     |   7.95   \n",
            "   2    |   920   |   0.306884   |     -      |     -     |   7.96   \n",
            "   2    |   940   |   0.003799   |     -      |     -     |   7.99   \n",
            "   2    |   960   |   0.548480   |     -      |     -     |   7.96   \n",
            "   2    |   980   |   0.080437   |     -      |     -     |   7.96   \n",
            "   2    |  1000   |   0.143581   |     -      |     -     |   7.99   \n",
            "   2    |  1020   |   0.142746   |     -      |     -     |   7.97   \n",
            "   2    |  1040   |   0.164896   |     -      |     -     |   7.98   \n",
            "   2    |  1060   |   0.062544   |     -      |     -     |   7.98   \n",
            "   2    |  1080   |   0.201827   |     -      |     -     |   7.96   \n",
            "   2    |  1100   |   0.259375   |     -      |     -     |   7.99   \n",
            "   2    |  1120   |   0.269783   |     -      |     -     |   7.96   \n",
            "   2    |  1140   |   0.130049   |     -      |     -     |   7.99   \n",
            "   2    |  1160   |   0.389399   |     -      |     -     |   7.98   \n",
            "   2    |  1180   |   0.309763   |     -      |     -     |   7.98   \n",
            "   2    |  1200   |   0.185814   |     -      |     -     |   7.96   \n",
            "   2    |  1220   |   0.238679   |     -      |     -     |   7.95   \n",
            "   2    |  1240   |   0.181209   |     -      |     -     |   7.96   \n",
            "   2    |  1260   |   0.205847   |     -      |     -     |   7.96   \n",
            "   2    |  1280   |   0.102029   |     -      |     -     |   7.98   \n",
            "   2    |  1300   |   0.147481   |     -      |     -     |   7.95   \n",
            "   2    |  1320   |   0.218176   |     -      |     -     |   7.99   \n",
            "   2    |  1340   |   0.221849   |     -      |     -     |   8.02   \n",
            "   2    |  1360   |   0.167551   |     -      |     -     |   7.99   \n",
            "   2    |  1380   |   0.212906   |     -      |     -     |   7.97   \n",
            "   2    |  1400   |   0.423834   |     -      |     -     |   7.99   \n",
            "   2    |  1420   |   0.402502   |     -      |     -     |   8.00   \n",
            "   2    |  1440   |   0.381207   |     -      |     -     |   8.00   \n",
            "   2    |  1460   |   0.233968   |     -      |     -     |   7.98   \n",
            "   2    |  1480   |   0.132493   |     -      |     -     |   8.03   \n",
            "   2    |  1500   |   0.135517   |     -      |     -     |   7.97   \n",
            "   2    |  1520   |   0.102888   |     -      |     -     |   7.99   \n",
            "   2    |  1540   |   0.493192   |     -      |     -     |   7.99   \n",
            "   2    |  1560   |   0.286141   |     -      |     -     |   7.98   \n",
            "   2    |  1580   |   0.236691   |     -      |     -     |   7.97   \n",
            "   2    |  1600   |   0.196601   |     -      |     -     |   7.98   \n",
            "   2    |  1620   |   0.240931   |     -      |     -     |   8.01   \n",
            "   2    |  1640   |   0.267601   |     -      |     -     |   7.98   \n",
            "   2    |  1660   |   0.298005   |     -      |     -     |   8.00   \n",
            "   2    |  1680   |   0.552806   |     -      |     -     |   7.99   \n",
            "   2    |  1700   |   0.307396   |     -      |     -     |   8.00   \n",
            "   2    |  1720   |   0.136418   |     -      |     -     |   7.96   \n",
            "   2    |  1740   |   0.323757   |     -      |     -     |   7.98   \n",
            "   2    |  1760   |   0.140448   |     -      |     -     |   7.93   \n",
            "   2    |  1780   |   0.257016   |     -      |     -     |   7.98   \n",
            "   2    |  1800   |   0.409368   |     -      |     -     |   7.97   \n",
            "   2    |  1820   |   0.247339   |     -      |     -     |   7.98   \n",
            "   2    |  1840   |   0.225626   |     -      |     -     |   7.96   \n",
            "   2    |  1860   |   0.301149   |     -      |     -     |   7.98   \n",
            "   2    |  1880   |   0.257560   |     -      |     -     |   8.01   \n",
            "   2    |  1900   |   0.124331   |     -      |     -     |   7.96   \n",
            "   2    |  1920   |   0.220521   |     -      |     -     |   7.95   \n",
            "   2    |  1940   |   0.224396   |     -      |     -     |   7.95   \n",
            "   2    |  1960   |   0.195136   |     -      |     -     |   7.95   \n",
            "   2    |  1980   |   0.238048   |     -      |     -     |   7.97   \n",
            "   2    |  2000   |   0.307569   |     -      |     -     |   7.97   \n",
            "   2    |  2020   |   0.330618   |     -      |     -     |   7.98   \n",
            "   2    |  2040   |   0.234881   |     -      |     -     |   7.92   \n",
            "   2    |  2060   |   0.077073   |     -      |     -     |   7.95   \n",
            "   2    |  2080   |   0.078441   |     -      |     -     |   7.95   \n",
            "   2    |  2100   |   0.137808   |     -      |     -     |   7.96   \n",
            "   2    |  2120   |   0.081373   |     -      |     -     |   7.99   \n",
            "   2    |  2140   |   0.224084   |     -      |     -     |   7.95   \n",
            "   2    |  2160   |   0.064209   |     -      |     -     |   7.93   \n",
            "   2    |  2180   |   0.338817   |     -      |     -     |   8.00   \n",
            "   2    |  2200   |   0.333157   |     -      |     -     |   7.96   \n",
            "   2    |  2220   |   0.132625   |     -      |     -     |   7.97   \n",
            "   2    |  2240   |   0.122349   |     -      |     -     |   7.95   \n",
            "   2    |  2260   |   0.235060   |     -      |     -     |   7.97   \n",
            "   2    |  2280   |   0.196060   |     -      |     -     |   7.93   \n",
            "   2    |  2300   |   0.582804   |     -      |     -     |   8.01   \n",
            "   2    |  2320   |   0.125230   |     -      |     -     |   8.00   \n",
            "   2    |  2340   |   0.193915   |     -      |     -     |   7.96   \n",
            "   2    |  2360   |   0.333528   |     -      |     -     |   7.95   \n",
            "   2    |  2380   |   0.190826   |     -      |     -     |   7.99   \n",
            "   2    |  2400   |   0.176447   |     -      |     -     |   7.93   \n",
            "   2    |  2420   |   0.227533   |     -      |     -     |   8.00   \n",
            "   2    |  2440   |   0.474863   |     -      |     -     |   7.99   \n",
            "   2    |  2460   |   0.137234   |     -      |     -     |   7.96   \n",
            "   2    |  2480   |   0.180905   |     -      |     -     |   7.96   \n",
            "   2    |  2500   |   0.273208   |     -      |     -     |   7.99   \n",
            "   2    |  2520   |   0.315141   |     -      |     -     |   8.00   \n",
            "   2    |  2540   |   0.238075   |     -      |     -     |   7.97   \n",
            "   2    |  2560   |   0.371332   |     -      |     -     |   8.00   \n",
            "   2    |  2580   |   0.204011   |     -      |     -     |   7.97   \n",
            "   2    |  2600   |   0.176890   |     -      |     -     |   8.00   \n",
            "   2    |  2620   |   0.120724   |     -      |     -     |   7.95   \n",
            "   2    |  2640   |   0.173375   |     -      |     -     |   7.98   \n",
            "   2    |  2660   |   0.182941   |     -      |     -     |   7.95   \n",
            "   2    |  2680   |   0.186786   |     -      |     -     |   7.92   \n",
            "   2    |  2700   |   0.074277   |     -      |     -     |   7.99   \n",
            "   2    |  2720   |   0.052171   |     -      |     -     |   7.96   \n",
            "   2    |  2740   |   0.198331   |     -      |     -     |   7.97   \n",
            "   2    |  2760   |   0.085534   |     -      |     -     |   7.99   \n",
            "   2    |  2780   |   0.353517   |     -      |     -     |   7.98   \n",
            "   2    |  2800   |   0.005016   |     -      |     -     |   7.98   \n",
            "   2    |  2820   |   0.167536   |     -      |     -     |   7.96   \n",
            "   2    |  2840   |   0.409355   |     -      |     -     |   7.96   \n",
            "   2    |  2860   |   0.064928   |     -      |     -     |   7.97   \n",
            "   2    |  2880   |   0.123233   |     -      |     -     |   7.99   \n",
            "   2    |  2900   |   0.215739   |     -      |     -     |   8.00   \n",
            "   2    |  2920   |   0.371364   |     -      |     -     |   7.94   \n",
            "   2    |  2940   |   0.058952   |     -      |     -     |   7.98   \n",
            "   2    |  2960   |   0.278308   |     -      |     -     |   8.02   \n",
            "   2    |  2980   |   0.303944   |     -      |     -     |   7.96   \n",
            "   2    |  3000   |   0.165446   |     -      |     -     |   8.00   \n",
            "   2    |  3020   |   0.206986   |     -      |     -     |   7.94   \n",
            "   2    |  3040   |   0.192191   |     -      |     -     |   7.94   \n",
            "   2    |  3060   |   0.110526   |     -      |     -     |   7.97   \n",
            "   2    |  3080   |   0.148504   |     -      |     -     |   7.99   \n",
            "   2    |  3100   |   0.202181   |     -      |     -     |   7.95   \n",
            "   2    |  3120   |   0.394474   |     -      |     -     |   7.97   \n",
            "   2    |  3140   |   0.063324   |     -      |     -     |   8.00   \n",
            "   2    |  3160   |   0.137459   |     -      |     -     |   7.98   \n",
            "   2    |  3180   |   0.253824   |     -      |     -     |   7.98   \n",
            "   2    |  3200   |   0.124722   |     -      |     -     |   8.00   \n",
            "   2    |  3220   |   0.047321   |     -      |     -     |   7.96   \n",
            "   2    |  3240   |   0.192358   |     -      |     -     |   7.99   \n",
            "   2    |  3260   |   0.306733   |     -      |     -     |   7.96   \n",
            "   2    |  3280   |   0.036407   |     -      |     -     |   7.97   \n",
            "   2    |  3300   |   0.253965   |     -      |     -     |   7.98   \n",
            "   2    |  3320   |   0.135609   |     -      |     -     |   7.99   \n",
            "   2    |  3340   |   0.002086   |     -      |     -     |   7.96   \n",
            "   2    |  3360   |   0.065928   |     -      |     -     |   7.98   \n",
            "   2    |  3380   |   0.417055   |     -      |     -     |   7.98   \n",
            "   2    |  3400   |   0.151906   |     -      |     -     |   7.96   \n",
            "   2    |  3420   |   0.285363   |     -      |     -     |   7.97   \n",
            "   2    |  3440   |   0.237621   |     -      |     -     |   7.97   \n",
            "   2    |  3460   |   0.188388   |     -      |     -     |   7.99   \n",
            "   2    |  3480   |   0.394586   |     -      |     -     |   8.00   \n",
            "   2    |  3500   |   0.310416   |     -      |     -     |   7.98   \n",
            "   2    |  3520   |   0.312013   |     -      |     -     |   7.96   \n",
            "   2    |  3540   |   0.363078   |     -      |     -     |   7.97   \n",
            "   2    |  3560   |   0.152935   |     -      |     -     |   7.97   \n",
            "   2    |  3580   |   0.378129   |     -      |     -     |   7.97   \n",
            "   2    |  3600   |   0.183353   |     -      |     -     |   7.98   \n",
            "   2    |  3620   |   0.089769   |     -      |     -     |   8.02   \n",
            "   2    |  3640   |   0.219907   |     -      |     -     |   7.98   \n",
            "   2    |  3660   |   0.143913   |     -      |     -     |   7.98   \n",
            "   2    |  3680   |   0.209002   |     -      |     -     |   8.00   \n",
            "   2    |  3700   |   0.124286   |     -      |     -     |   8.01   \n",
            "   2    |  3720   |   0.210308   |     -      |     -     |   7.96   \n",
            "   2    |  3740   |   0.118161   |     -      |     -     |   7.97   \n",
            "   2    |  3760   |   0.409388   |     -      |     -     |   7.97   \n",
            "   2    |  3780   |   0.256342   |     -      |     -     |   7.97   \n",
            "   2    |  3800   |   0.624360   |     -      |     -     |   7.96   \n",
            "   2    |  3820   |   0.311290   |     -      |     -     |   7.96   \n",
            "   2    |  3840   |   0.102419   |     -      |     -     |   7.96   \n",
            "   2    |  3860   |   0.271847   |     -      |     -     |   7.96   \n",
            "   2    |  3880   |   0.155973   |     -      |     -     |   7.99   \n",
            "   2    |  3900   |   0.118373   |     -      |     -     |   7.95   \n",
            "   2    |  3920   |   0.281677   |     -      |     -     |   7.99   \n",
            "   2    |  3940   |   0.173712   |     -      |     -     |   7.93   \n",
            "   2    |  3960   |   0.131134   |     -      |     -     |   7.97   \n",
            "   2    |  3980   |   0.338984   |     -      |     -     |   7.98   \n",
            "   2    |  4000   |   0.282243   |     -      |     -     |   7.95   \n",
            "   2    |  4020   |   0.333803   |     -      |     -     |   7.97   \n",
            "   2    |  4040   |   0.240917   |     -      |     -     |   7.99   \n",
            "   2    |  4060   |   0.129279   |     -      |     -     |   7.96   \n",
            "   2    |  4080   |   0.212177   |     -      |     -     |   7.98   \n",
            "   2    |  4100   |   0.176551   |     -      |     -     |   7.95   \n",
            "   2    |  4120   |   0.178791   |     -      |     -     |   7.96   \n",
            "   2    |  4140   |   0.276733   |     -      |     -     |   7.95   \n",
            "   2    |  4160   |   0.174631   |     -      |     -     |   7.96   \n",
            "   2    |  4180   |   0.126922   |     -      |     -     |   7.96   \n",
            "   2    |  4200   |   0.072036   |     -      |     -     |   7.97   \n",
            "   2    |  4220   |   0.059887   |     -      |     -     |   7.95   \n",
            "   2    |  4240   |   0.213773   |     -      |     -     |   7.95   \n",
            "   2    |  4260   |   0.160034   |     -      |     -     |   7.97   \n",
            "   2    |  4280   |   0.219454   |     -      |     -     |   7.98   \n",
            "   2    |  4300   |   0.126972   |     -      |     -     |   7.97   \n",
            "   2    |  4320   |   0.259604   |     -      |     -     |   7.99   \n",
            "   2    |  4340   |   0.075090   |     -      |     -     |   8.00   \n",
            "   2    |  4360   |   0.472336   |     -      |     -     |   7.99   \n",
            "   2    |  4380   |   0.264003   |     -      |     -     |   7.99   \n",
            "   2    |  4400   |   0.305937   |     -      |     -     |   7.97   \n",
            "   2    |  4420   |   0.064769   |     -      |     -     |   7.99   \n",
            "   2    |  4440   |   0.072836   |     -      |     -     |   7.96   \n",
            "   2    |  4460   |   0.047690   |     -      |     -     |   8.02   \n",
            "   2    |  4480   |   0.273306   |     -      |     -     |   8.04   \n",
            "   2    |  4500   |   0.359141   |     -      |     -     |   7.97   \n",
            "   2    |  4520   |   0.219188   |     -      |     -     |   7.96   \n",
            "   2    |  4540   |   0.133723   |     -      |     -     |   8.00   \n",
            "   2    |  4560   |   0.274207   |     -      |     -     |   8.00   \n",
            "   2    |  4580   |   0.488247   |     -      |     -     |   7.96   \n",
            "   2    |  4600   |   0.063022   |     -      |     -     |   8.00   \n",
            "   2    |  4620   |   0.285758   |     -      |     -     |   7.97   \n",
            "   2    |  4640   |   0.178081   |     -      |     -     |   7.98   \n",
            "   2    |  4660   |   0.158868   |     -      |     -     |   7.95   \n",
            "   2    |  4680   |   0.401439   |     -      |     -     |   7.96   \n",
            "   2    |  4700   |   0.118205   |     -      |     -     |   7.98   \n",
            "   2    |  4720   |   0.246138   |     -      |     -     |   7.98   \n",
            "   2    |  4740   |   0.059246   |     -      |     -     |   7.98   \n",
            "   2    |  4760   |   0.434909   |     -      |     -     |   8.01   \n",
            "   2    |  4780   |   0.050929   |     -      |     -     |   7.98   \n",
            "   2    |  4800   |   0.302209   |     -      |     -     |   8.00   \n",
            "   2    |  4820   |   0.326317   |     -      |     -     |   7.98   \n",
            "   2    |  4840   |   0.190501   |     -      |     -     |   7.98   \n",
            "   2    |  4860   |   0.149362   |     -      |     -     |   7.95   \n",
            "   2    |  4880   |   0.083428   |     -      |     -     |   7.97   \n",
            "   2    |  4900   |   0.252222   |     -      |     -     |   7.95   \n",
            "   2    |  4920   |   0.094013   |     -      |     -     |   7.95   \n",
            "   2    |  4940   |   0.245062   |     -      |     -     |   7.97   \n",
            "   2    |  4960   |   0.211160   |     -      |     -     |   7.93   \n",
            "   2    |  4980   |   0.258293   |     -      |     -     |   7.96   \n",
            "   2    |  5000   |   0.261444   |     -      |     -     |   7.97   \n",
            "   2    |  5020   |   0.103617   |     -      |     -     |   8.02   \n",
            "   2    |  5040   |   0.252068   |     -      |     -     |   7.98   \n",
            "   2    |  5060   |   0.002381   |     -      |     -     |   7.99   \n",
            "   2    |  5080   |   0.100744   |     -      |     -     |   7.98   \n",
            "   2    |  5100   |   0.247810   |     -      |     -     |   7.94   \n",
            "   2    |  5120   |   0.351745   |     -      |     -     |   7.98   \n",
            "   2    |  5140   |   0.071456   |     -      |     -     |   7.97   \n",
            "   2    |  5160   |   0.111985   |     -      |     -     |   7.97   \n",
            "   2    |  5180   |   0.127533   |     -      |     -     |   7.94   \n",
            "   2    |  5200   |   0.283576   |     -      |     -     |   7.97   \n",
            "   2    |  5220   |   0.110661   |     -      |     -     |   7.96   \n",
            "   2    |  5240   |   0.185754   |     -      |     -     |   8.01   \n",
            "   2    |  5260   |   0.149552   |     -      |     -     |   7.99   \n",
            "   2    |  5280   |   0.151695   |     -      |     -     |   7.98   \n",
            "   2    |  5300   |   0.264176   |     -      |     -     |   7.96   \n",
            "   2    |  5320   |   0.193635   |     -      |     -     |   7.97   \n",
            "   2    |  5340   |   0.289565   |     -      |     -     |   8.00   \n",
            "   2    |  5360   |   0.534436   |     -      |     -     |   7.98   \n",
            "   2    |  5380   |   0.121658   |     -      |     -     |   7.97   \n",
            "   2    |  5400   |   0.084859   |     -      |     -     |   8.00   \n",
            "   2    |  5420   |   0.264362   |     -      |     -     |   7.98   \n",
            "   2    |  5440   |   0.225370   |     -      |     -     |   7.97   \n",
            "   2    |  5460   |   0.002618   |     -      |     -     |   7.98   \n",
            "   2    |  5480   |   0.250919   |     -      |     -     |   7.98   \n",
            "   2    |  5500   |   0.192387   |     -      |     -     |   7.97   \n",
            "   2    |  5520   |   0.213510   |     -      |     -     |   7.96   \n",
            "   2    |  5540   |   0.151913   |     -      |     -     |   7.93   \n",
            "   2    |  5560   |   0.132725   |     -      |     -     |   7.99   \n",
            "   2    |  5580   |   0.167156   |     -      |     -     |   7.97   \n",
            "   2    |  5600   |   0.275641   |     -      |     -     |   7.99   \n",
            "   2    |  5620   |   0.535596   |     -      |     -     |   7.99   \n",
            "   2    |  5640   |   0.132132   |     -      |     -     |   7.97   \n",
            "   2    |  5660   |   0.225719   |     -      |     -     |   7.96   \n",
            "   2    |  5680   |   0.253936   |     -      |     -     |   8.00   \n",
            "   2    |  5700   |   0.203176   |     -      |     -     |   7.97   \n",
            "   2    |  5720   |   0.259036   |     -      |     -     |   8.00   \n",
            "   2    |  5740   |   0.068369   |     -      |     -     |   7.97   \n",
            "   2    |  5760   |   0.249001   |     -      |     -     |   8.00   \n",
            "   2    |  5780   |   0.103111   |     -      |     -     |   7.99   \n",
            "   2    |  5800   |   0.189445   |     -      |     -     |   7.95   \n",
            "   2    |  5820   |   0.181680   |     -      |     -     |   7.96   \n",
            "   2    |  5840   |   0.422108   |     -      |     -     |   7.96   \n",
            "   2    |  5860   |   0.179205   |     -      |     -     |   7.97   \n",
            "   2    |  5880   |   0.206486   |     -      |     -     |   7.96   \n",
            "   2    |  5900   |   0.267449   |     -      |     -     |   8.00   \n",
            "   2    |  5920   |   0.184558   |     -      |     -     |   7.96   \n",
            "   2    |  5940   |   0.145095   |     -      |     -     |   8.00   \n",
            "   2    |  5960   |   0.189767   |     -      |     -     |   7.95   \n",
            "   2    |  5980   |   0.005103   |     -      |     -     |   7.99   \n",
            "   2    |  6000   |   0.121737   |     -      |     -     |   7.99   \n",
            "   2    |  6020   |   0.124870   |     -      |     -     |   7.96   \n",
            "   2    |  6040   |   0.168197   |     -      |     -     |   7.96   \n",
            "   2    |  6060   |   0.115705   |     -      |     -     |   7.95   \n",
            "   2    |  6080   |   0.115135   |     -      |     -     |   7.99   \n",
            "   2    |  6100   |   0.104694   |     -      |     -     |   8.00   \n",
            "   2    |  6120   |   0.195832   |     -      |     -     |   7.98   \n",
            "   2    |  6140   |   0.007949   |     -      |     -     |   7.96   \n",
            "   2    |  6160   |   0.159886   |     -      |     -     |   8.01   \n",
            "   2    |  6180   |   0.190490   |     -      |     -     |   7.97   \n",
            "   2    |  6200   |   0.202509   |     -      |     -     |   8.00   \n",
            "   2    |  6220   |   0.305915   |     -      |     -     |   7.97   \n",
            "   2    |  6240   |   0.341320   |     -      |     -     |   7.98   \n",
            "   2    |  6260   |   0.121909   |     -      |     -     |   7.97   \n",
            "   2    |  6280   |   0.114843   |     -      |     -     |   7.95   \n",
            "   2    |  6300   |   0.308674   |     -      |     -     |   7.99   \n",
            "   2    |  6320   |   0.406458   |     -      |     -     |   7.98   \n",
            "   2    |  6340   |   0.121907   |     -      |     -     |   7.97   \n",
            "   2    |  6360   |   0.139828   |     -      |     -     |   7.98   \n",
            "   2    |  6380   |   0.067961   |     -      |     -     |   8.00   \n",
            "   2    |  6400   |   0.152982   |     -      |     -     |   8.02   \n",
            "   2    |  6420   |   0.262986   |     -      |     -     |   8.00   \n",
            "   2    |  6440   |   0.143838   |     -      |     -     |   7.96   \n",
            "   2    |  6460   |   0.227500   |     -      |     -     |   7.95   \n",
            "   2    |  6480   |   0.441136   |     -      |     -     |   8.01   \n",
            "   2    |  6500   |   0.147096   |     -      |     -     |   7.98   \n",
            "   2    |  6520   |   0.118958   |     -      |     -     |   7.97   \n",
            "   2    |  6540   |   0.291799   |     -      |     -     |   7.95   \n",
            "   2    |  6560   |   0.151831   |     -      |     -     |   7.95   \n",
            "   2    |  6580   |   0.128453   |     -      |     -     |   7.95   \n",
            "   2    |  6600   |   0.126125   |     -      |     -     |   7.96   \n",
            "   2    |  6620   |   0.195917   |     -      |     -     |   7.94   \n",
            "   2    |  6640   |   0.043340   |     -      |     -     |   7.97   \n",
            "   2    |  6660   |   0.111936   |     -      |     -     |   7.96   \n",
            "   2    |  6680   |   0.238366   |     -      |     -     |   7.93   \n",
            "   2    |  6700   |   0.218581   |     -      |     -     |   7.96   \n",
            "   2    |  6720   |   0.286272   |     -      |     -     |   7.97   \n",
            "   2    |  6740   |   0.214236   |     -      |     -     |   7.95   \n",
            "   2    |  6760   |   0.286900   |     -      |     -     |   7.96   \n",
            "   2    |  6780   |   0.131736   |     -      |     -     |   7.95   \n",
            "   2    |  6800   |   0.179950   |     -      |     -     |   7.97   \n",
            "   2    |  6820   |   0.043688   |     -      |     -     |   7.96   \n",
            "   2    |  6840   |   0.219656   |     -      |     -     |   7.99   \n",
            "   2    |  6860   |   0.288839   |     -      |     -     |   7.96   \n",
            "   2    |  6880   |   0.143210   |     -      |     -     |   7.96   \n",
            "   2    |  6900   |   0.178282   |     -      |     -     |   7.98   \n",
            "   2    |  6920   |   0.214864   |     -      |     -     |   7.97   \n",
            "   2    |  6940   |   0.001746   |     -      |     -     |   7.97   \n",
            "   2    |  6960   |   0.285248   |     -      |     -     |   7.94   \n",
            "   2    |  6980   |   0.088725   |     -      |     -     |   7.97   \n",
            "   2    |  7000   |   0.136273   |     -      |     -     |   7.97   \n",
            "   2    |  7020   |   0.257777   |     -      |     -     |   7.99   \n",
            "   2    |  7040   |   0.168554   |     -      |     -     |   8.00   \n",
            "   2    |  7060   |   0.417695   |     -      |     -     |   8.02   \n",
            "   2    |  7080   |   0.164860   |     -      |     -     |   7.95   \n",
            "   2    |  7100   |   0.189189   |     -      |     -     |   7.94   \n",
            "   2    |  7120   |   0.083980   |     -      |     -     |   7.97   \n",
            "   2    |  7140   |   0.211623   |     -      |     -     |   7.95   \n",
            "   2    |  7160   |   0.167066   |     -      |     -     |   7.98   \n",
            "   2    |  7180   |   0.129241   |     -      |     -     |   7.93   \n",
            "   2    |  7200   |   0.261271   |     -      |     -     |   7.94   \n",
            "   2    |  7220   |   0.077125   |     -      |     -     |   7.95   \n",
            "   2    |  7240   |   0.238575   |     -      |     -     |   7.95   \n",
            "   2    |  7260   |   0.213639   |     -      |     -     |   7.99   \n",
            "   2    |  7280   |   0.112525   |     -      |     -     |   7.96   \n",
            "   2    |  7300   |   0.190583   |     -      |     -     |   7.99   \n",
            "   2    |  7320   |   0.354802   |     -      |     -     |   7.98   \n",
            "   2    |  7340   |   0.313561   |     -      |     -     |   7.98   \n",
            "   2    |  7360   |   0.184923   |     -      |     -     |   8.00   \n",
            "   2    |  7380   |   0.085621   |     -      |     -     |   7.98   \n",
            "   2    |  7400   |   0.209161   |     -      |     -     |   7.98   \n",
            "   2    |  7420   |   0.251071   |     -      |     -     |   7.97   \n",
            "   2    |  7440   |   0.456838   |     -      |     -     |   8.00   \n",
            "   2    |  7460   |   0.324958   |     -      |     -     |   7.98   \n",
            "   2    |  7480   |   0.147741   |     -      |     -     |   7.98   \n",
            "   2    |  7500   |   0.229360   |     -      |     -     |   7.98   \n",
            "   2    |  7520   |   0.155380   |     -      |     -     |   7.94   \n",
            "   2    |  7540   |   0.003880   |     -      |     -     |   7.97   \n",
            "   2    |  7560   |   0.136910   |     -      |     -     |   7.96   \n",
            "   2    |  7580   |   0.123825   |     -      |     -     |   8.00   \n",
            "   2    |  7600   |   0.053698   |     -      |     -     |   8.03   \n",
            "   2    |  7620   |   0.250790   |     -      |     -     |   7.96   \n",
            "   2    |  7640   |   0.271676   |     -      |     -     |   8.00   \n",
            "   2    |  7660   |   0.038020   |     -      |     -     |   7.95   \n",
            "   2    |  7680   |   0.059277   |     -      |     -     |   7.99   \n",
            "   2    |  7700   |   0.053896   |     -      |     -     |   7.96   \n",
            "   2    |  7720   |   0.367714   |     -      |     -     |   7.96   \n",
            "   2    |  7740   |   0.178395   |     -      |     -     |   7.97   \n",
            "   2    |  7760   |   0.015058   |     -      |     -     |   7.95   \n",
            "   2    |  7780   |   0.222522   |     -      |     -     |   7.97   \n",
            "   2    |  7800   |   0.339088   |     -      |     -     |   7.98   \n",
            "   2    |  7820   |   0.411034   |     -      |     -     |   7.99   \n",
            "   2    |  7840   |   0.002277   |     -      |     -     |   7.99   \n",
            "   2    |  7860   |   0.280602   |     -      |     -     |   7.96   \n",
            "   2    |  7880   |   0.248305   |     -      |     -     |   7.97   \n",
            "   2    |  7900   |   0.058599   |     -      |     -     |   8.02   \n",
            "   2    |  7920   |   0.261242   |     -      |     -     |   7.99   \n",
            "   2    |  7940   |   0.089159   |     -      |     -     |   7.97   \n",
            "   2    |  7960   |   0.189238   |     -      |     -     |   7.97   \n",
            "   2    |  7980   |   0.169826   |     -      |     -     |   7.99   \n",
            "   2    |  8000   |   0.355546   |     -      |     -     |   8.01   \n",
            "   2    |  8020   |   0.472879   |     -      |     -     |   7.98   \n",
            "   2    |  8040   |   0.138206   |     -      |     -     |   7.97   \n",
            "   2    |  8060   |   0.056122   |     -      |     -     |   7.99   \n",
            "   2    |  8080   |   0.124985   |     -      |     -     |   7.98   \n",
            "   2    |  8100   |   0.095787   |     -      |     -     |   7.99   \n",
            "   2    |  8120   |   0.300265   |     -      |     -     |   7.97   \n",
            "   2    |  8140   |   0.071232   |     -      |     -     |   7.99   \n",
            "   2    |  8160   |   0.086267   |     -      |     -     |   7.95   \n",
            "   2    |  8180   |   0.287313   |     -      |     -     |   7.96   \n",
            "   2    |  8200   |   0.160318   |     -      |     -     |   8.01   \n",
            "   2    |  8220   |   0.104960   |     -      |     -     |   7.95   \n",
            "   2    |  8240   |   0.002442   |     -      |     -     |   7.94   \n",
            "   2    |  8260   |   0.288270   |     -      |     -     |   7.95   \n",
            "   2    |  8280   |   0.202386   |     -      |     -     |   7.98   \n",
            "   2    |  8300   |   0.186233   |     -      |     -     |   7.98   \n",
            "   2    |  8320   |   0.088290   |     -      |     -     |   7.98   \n",
            "   2    |  8340   |   0.319072   |     -      |     -     |   7.96   \n",
            "   2    |  8360   |   0.085844   |     -      |     -     |   7.99   \n",
            "   2    |  8380   |   0.125837   |     -      |     -     |   7.97   \n",
            "   2    |  8400   |   0.078547   |     -      |     -     |   7.96   \n",
            "   2    |  8420   |   0.073531   |     -      |     -     |   7.96   \n",
            "   2    |  8440   |   0.193402   |     -      |     -     |   7.96   \n",
            "   2    |  8460   |   0.247826   |     -      |     -     |   7.97   \n",
            "   2    |  8480   |   0.298790   |     -      |     -     |   7.99   \n",
            "   2    |  8500   |   0.163627   |     -      |     -     |   7.98   \n",
            "   2    |  8520   |   0.210577   |     -      |     -     |   7.95   \n",
            "   2    |  8540   |   0.328387   |     -      |     -     |   7.98   \n",
            "   2    |  8560   |   0.173490   |     -      |     -     |   7.96   \n",
            "   2    |  8580   |   0.077090   |     -      |     -     |   7.97   \n",
            "   2    |  8600   |   0.087501   |     -      |     -     |   8.00   \n",
            "   2    |  8620   |   0.002191   |     -      |     -     |   7.98   \n",
            "   2    |  8640   |   0.325074   |     -      |     -     |   7.96   \n",
            "   2    |  8660   |   0.281549   |     -      |     -     |   7.97   \n",
            "   2    |  8680   |   0.195984   |     -      |     -     |   8.01   \n",
            "   2    |  8700   |   0.054901   |     -      |     -     |   7.95   \n",
            "   2    |  8720   |   0.062830   |     -      |     -     |   7.96   \n",
            "   2    |  8740   |   0.123379   |     -      |     -     |   7.95   \n",
            "   2    |  8760   |   0.134127   |     -      |     -     |   8.00   \n",
            "   2    |  8780   |   0.114537   |     -      |     -     |   7.98   \n",
            "   2    |  8800   |   0.246925   |     -      |     -     |   7.96   \n",
            "   2    |  8820   |   0.094684   |     -      |     -     |   7.96   \n",
            "   2    |  8840   |   0.202110   |     -      |     -     |   7.97   \n",
            "   2    |  8860   |   0.183727   |     -      |     -     |   7.97   \n",
            "   2    |  8880   |   0.067844   |     -      |     -     |   7.98   \n",
            "   2    |  8900   |   0.125767   |     -      |     -     |   7.98   \n",
            "   2    |  8920   |   0.141956   |     -      |     -     |   7.95   \n",
            "   2    |  8940   |   0.275956   |     -      |     -     |   7.97   \n",
            "   2    |  8960   |   0.064993   |     -      |     -     |   7.98   \n",
            "   2    |  8980   |   0.129380   |     -      |     -     |   7.95   \n",
            "   2    |  9000   |   0.175655   |     -      |     -     |   7.95   \n",
            "   2    |  9020   |   0.064683   |     -      |     -     |   7.99   \n",
            "   2    |  9040   |   0.311184   |     -      |     -     |   7.98   \n",
            "   2    |  9060   |   0.301941   |     -      |     -     |   7.95   \n",
            "   2    |  9080   |   0.123305   |     -      |     -     |   7.95   \n",
            "   2    |  9100   |   0.268695   |     -      |     -     |   7.98   \n",
            "   2    |  9120   |   0.208067   |     -      |     -     |   7.98   \n",
            "   2    |  9140   |   0.201370   |     -      |     -     |   7.95   \n",
            "   2    |  9160   |   0.269072   |     -      |     -     |   7.96   \n",
            "   2    |  9180   |   0.140186   |     -      |     -     |   7.96   \n",
            "   2    |  9200   |   0.200445   |     -      |     -     |   7.96   \n",
            "   2    |  9220   |   0.282788   |     -      |     -     |   7.97   \n",
            "   2    |  9240   |   0.214187   |     -      |     -     |   7.97   \n",
            "   2    |  9260   |   0.220903   |     -      |     -     |   7.99   \n",
            "   2    |  9280   |   0.300585   |     -      |     -     |   7.96   \n",
            "   2    |  9300   |   0.202663   |     -      |     -     |   7.95   \n",
            "   2    |  9320   |   0.080809   |     -      |     -     |   7.96   \n",
            "   2    |  9340   |   0.438613   |     -      |     -     |   7.96   \n",
            "   2    |  9360   |   0.096285   |     -      |     -     |   7.95   \n",
            "   2    |  9380   |   0.146688   |     -      |     -     |   8.00   \n",
            "   2    |  9400   |   0.059570   |     -      |     -     |   7.96   \n",
            "   2    |  9420   |   0.270549   |     -      |     -     |   7.95   \n",
            "   2    |  9440   |   0.108137   |     -      |     -     |   8.01   \n",
            "   2    |  9460   |   0.153275   |     -      |     -     |   8.00   \n",
            "   2    |  9480   |   0.332859   |     -      |     -     |   7.97   \n",
            "   2    |  9500   |   0.144005   |     -      |     -     |   7.97   \n",
            "   2    |  9520   |   0.058783   |     -      |     -     |   8.01   \n",
            "   2    |  9540   |   0.169730   |     -      |     -     |   7.97   \n",
            "   2    |  9560   |   0.176640   |     -      |     -     |   7.96   \n",
            "   2    |  9580   |   0.190452   |     -      |     -     |   8.00   \n",
            "   2    |  9600   |   0.321836   |     -      |     -     |   7.95   \n",
            "   2    |  9620   |   0.377183   |     -      |     -     |   7.97   \n",
            "   2    |  9640   |   0.350163   |     -      |     -     |   7.95   \n",
            "   2    |  9660   |   0.305203   |     -      |     -     |   7.94   \n",
            "   2    |  9680   |   0.295627   |     -      |     -     |   7.99   \n",
            "   2    |  9700   |   0.175832   |     -      |     -     |   7.99   \n",
            "   2    |  9720   |   0.193028   |     -      |     -     |   7.97   \n",
            "   2    |  9740   |   0.197413   |     -      |     -     |   7.97   \n",
            "   2    |  9760   |   0.148957   |     -      |     -     |   7.99   \n",
            "   2    |  9780   |   0.230930   |     -      |     -     |   7.97   \n",
            "   2    |  9800   |   0.021437   |     -      |     -     |   7.97   \n",
            "   2    |  9820   |   0.411088   |     -      |     -     |   7.98   \n",
            "   2    |  9840   |   0.296391   |     -      |     -     |   7.99   \n",
            "   2    |  9860   |   0.074128   |     -      |     -     |   7.97   \n",
            "   2    |  9880   |   0.253597   |     -      |     -     |   8.01   \n",
            "   2    |  9900   |   0.175899   |     -      |     -     |   7.99   \n",
            "   2    |  9920   |   0.304412   |     -      |     -     |   7.94   \n",
            "   2    |  9940   |   0.131299   |     -      |     -     |   7.98   \n",
            "   2    |  9960   |   0.110270   |     -      |     -     |   7.96   \n",
            "   2    |  9980   |   0.143751   |     -      |     -     |   7.98   \n",
            "   2    |  9999   |   0.074145   |     -      |     -     |   7.57   \n",
            "----------------------------------------------------------------------\n",
            "   2    |    -    |   0.201723   |  0.320856  |   93.56   |  4314.15 \n",
            "----------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Training complete!\n"
          ]
        }
      ],
      "source": [
        "#Now, let's start training our BertClassifier!\n",
        "torch.cuda.empty_cache()\n",
        "set_seed(42)    # Set seed for reproducibility\n",
        "bert_classifier, optimizer, scheduler = initialize_model(epochs=2)\n",
        "train(bert_classifier, train_dataloader, val_dataloader, epochs=2, evaluation=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJO4bxTOcsBk"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score, roc_curve, auc, precision_recall_fscore_support\n",
        "\n",
        "def evaluate_roc(probs, y_true):\n",
        "    \"\"\"\n",
        "    - Print AUC and accuracy on the test set\n",
        "    - Plot ROC\n",
        "    @params    probs (np.array): an array of predicted probabilities with shape (len(y_true), 2)\n",
        "    @params    y_true (np.array): an array of the true values with shape (len(y_true),)\n",
        "    \"\"\"\n",
        "    preds = probs[:, 1]\n",
        "\n",
        "\n",
        "\n",
        "    fpr, tpr, threshold = roc_curve(y_true, preds)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    print(f'AUC: {roc_auc:.4f}')\n",
        "\n",
        "    # Get accuracy over the test set\n",
        "    y_pred = np.where(preds >= 0.5, 1, 0)\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    print(f'Accuracy: {accuracy*100:.2f}%')\n",
        "\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    print(cm)\n",
        "    precision, recall, f1, support = precision_recall_fscore_support(y_true, y_pred, average = 'binary')\n",
        "    print(f\"precision:{precision:.3f} \\nrecall:{recall:.3f}\\nF1 score: {f1:.3f}\")\n",
        "\n",
        "    # Plot ROC AUC\n",
        "    plt.title('Receiver Operating Characteristic')\n",
        "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
        "    plt.legend(loc = 'lower right')\n",
        "    plt.plot([0, 1], [0, 1],'r--')\n",
        "    plt.xlim([0, 1])\n",
        "    plt.ylim([0, 1])\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82r-WdYzn76f"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def bert_predict(model, test_dataloader):\n",
        "    \"\"\"Perform a forward pass on the trained BERT model to predict probabilities\n",
        "    on the test set.\n",
        "    \"\"\"\n",
        "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
        "    # the test time.\n",
        "    model.eval()\n",
        "\n",
        "    all_logits = []\n",
        "\n",
        "    # For each batch in our test set...\n",
        "    for batch in test_dataloader:\n",
        "        # Load batch to GPU\n",
        "        b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]\n",
        "\n",
        "        # Compute logits\n",
        "        with torch.no_grad():\n",
        "            logits = model(b_input_ids, b_attn_mask)\n",
        "        all_logits.append(logits)\n",
        "\n",
        "    # Concatenate logits from each batch\n",
        "    all_logits = torch.cat(all_logits, dim=0)\n",
        "\n",
        "    # Apply softmax to calculate probabilities\n",
        "    probs = F.softmax(all_logits, dim=1).cpu().numpy()\n",
        "\n",
        "    return probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6JAfV6f3oEBd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "outputId": "4dd58d99-eb88-41ef-a7d6-25605034325d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC: 0.9829\n",
            "Accuracy: 93.56%\n",
            "[[4649  312]\n",
            " [ 332 4707]]\n",
            "precision:0.938 \n",
            "recall:0.934\n",
            "F1 score: 0.936\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Compute predicted probabilities on the test set\n",
        "probs = bert_predict(bert_classifier, val_dataloader)\n",
        "\n",
        "# Evaluate the Bert classifier\n",
        "evaluate_roc(probs, y_val)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2d0a50fb230e4fc7a4f391eb0efb55c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2f10f02cd1904522b3ad5c5a31e8bc06",
              "IPY_MODEL_24f940e557f4499dbf9de70b4e64de4f",
              "IPY_MODEL_987baedbdabf4a29b42379ad8126473b"
            ],
            "layout": "IPY_MODEL_50002f23763a41b38e7188be9b8623fb"
          }
        },
        "2f10f02cd1904522b3ad5c5a31e8bc06": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_19d9d15ebe2d46c1ac10c6b6b3157060",
            "placeholder": "​",
            "style": "IPY_MODEL_9b2ee64f724344fbad3ad380376a5f10",
            "value": "Downloading vocab.txt: 100%"
          }
        },
        "24f940e557f4499dbf9de70b4e64de4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d6da59d2b553438781e585250d4edb27",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a40667a6174443bda521e26b026d06ec",
            "value": 231508
          }
        },
        "987baedbdabf4a29b42379ad8126473b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_59664a7d9f864137b3013f2699492664",
            "placeholder": "​",
            "style": "IPY_MODEL_4b1e622beff84bf8b9e64df70bb8f196",
            "value": " 226k/226k [00:00&lt;00:00, 250kB/s]"
          }
        },
        "50002f23763a41b38e7188be9b8623fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19d9d15ebe2d46c1ac10c6b6b3157060": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b2ee64f724344fbad3ad380376a5f10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d6da59d2b553438781e585250d4edb27": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a40667a6174443bda521e26b026d06ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "59664a7d9f864137b3013f2699492664": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b1e622beff84bf8b9e64df70bb8f196": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4c720ea55a0b497dbf3c537cb1c1e227": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d153a9fefd654ba394939a17553bf787",
              "IPY_MODEL_41811c0955924f499246b15a4146130a",
              "IPY_MODEL_db34ef20b2c443778441d6b20a560f5d"
            ],
            "layout": "IPY_MODEL_5b596fdf7af4446d93b890f53bca933e"
          }
        },
        "d153a9fefd654ba394939a17553bf787": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9a396d0b3bfe4584a455fa3e4422fc5e",
            "placeholder": "​",
            "style": "IPY_MODEL_10cb97156ddf4d539e2ad91dbc2d436c",
            "value": "Downloading tokenizer_config.json: 100%"
          }
        },
        "41811c0955924f499246b15a4146130a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c3c4ddfa3424bf4b3516242884db39a",
            "max": 28,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1146f00b3fa943529b86e73fd477f82b",
            "value": 28
          }
        },
        "db34ef20b2c443778441d6b20a560f5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6cf5cbc5be0e4798844f33012bd6e34e",
            "placeholder": "​",
            "style": "IPY_MODEL_7370701388004662b38c44a082d92cbe",
            "value": " 28.0/28.0 [00:00&lt;00:00, 273B/s]"
          }
        },
        "5b596fdf7af4446d93b890f53bca933e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a396d0b3bfe4584a455fa3e4422fc5e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "10cb97156ddf4d539e2ad91dbc2d436c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5c3c4ddfa3424bf4b3516242884db39a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1146f00b3fa943529b86e73fd477f82b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6cf5cbc5be0e4798844f33012bd6e34e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7370701388004662b38c44a082d92cbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a74ae563f0ca4af5acd1f3e1023dccf1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_159606e3bfd647d38aaa525735d49a7c",
              "IPY_MODEL_e95aaa1b514848228ac17c258af9d2b3",
              "IPY_MODEL_e97773bf80d84d7a90350be8dd0ecbaf"
            ],
            "layout": "IPY_MODEL_8976e442c5274abc85076e3738848c61"
          }
        },
        "159606e3bfd647d38aaa525735d49a7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cf63af6978f645ce88f028db431116f0",
            "placeholder": "​",
            "style": "IPY_MODEL_d84fc750d9a34383b64120bdfc9ef299",
            "value": "Downloading config.json: 100%"
          }
        },
        "e95aaa1b514848228ac17c258af9d2b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e7b0d94931544c88ba84a1da23ed3758",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_60a83d8f267147e9be3d803dff3d25ef",
            "value": 570
          }
        },
        "e97773bf80d84d7a90350be8dd0ecbaf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f483cbf1d8d047ecac46ef899894cdaa",
            "placeholder": "​",
            "style": "IPY_MODEL_dbc1ac5b575d43f5868adeb1538ab94a",
            "value": " 570/570 [00:00&lt;00:00, 5.13kB/s]"
          }
        },
        "8976e442c5274abc85076e3738848c61": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf63af6978f645ce88f028db431116f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d84fc750d9a34383b64120bdfc9ef299": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e7b0d94931544c88ba84a1da23ed3758": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60a83d8f267147e9be3d803dff3d25ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f483cbf1d8d047ecac46ef899894cdaa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dbc1ac5b575d43f5868adeb1538ab94a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ac34aa20cdde4dc0bfc0acfcf12df439": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2df3de7381714f298d13d475b66c2d2f",
              "IPY_MODEL_2d98c56ef1a242549ad95bfb1f57d6a4",
              "IPY_MODEL_d2ced0cd2de944a2a6b252e9d644c787"
            ],
            "layout": "IPY_MODEL_4255f39cb0fe40adbdd2d4b2a579bad2"
          }
        },
        "2df3de7381714f298d13d475b66c2d2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7384f9e2b7df4888a4c9d5a0113a68d1",
            "placeholder": "​",
            "style": "IPY_MODEL_1823a63d42aa460a930b376f8aa5a720",
            "value": "Downloading pytorch_model.bin: 100%"
          }
        },
        "2d98c56ef1a242549ad95bfb1f57d6a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9cdf599b11b0495594325830e4b19973",
            "max": 440473133,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0e9126cdc9ff473481e4939956e2babd",
            "value": 440473133
          }
        },
        "d2ced0cd2de944a2a6b252e9d644c787": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_65f993a431c7417e883da7b70ec87eac",
            "placeholder": "​",
            "style": "IPY_MODEL_d95e13793956456aae04e0f782d7566f",
            "value": " 420M/420M [00:07&lt;00:00, 61.5MB/s]"
          }
        },
        "4255f39cb0fe40adbdd2d4b2a579bad2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7384f9e2b7df4888a4c9d5a0113a68d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1823a63d42aa460a930b376f8aa5a720": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9cdf599b11b0495594325830e4b19973": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e9126cdc9ff473481e4939956e2babd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "65f993a431c7417e883da7b70ec87eac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d95e13793956456aae04e0f782d7566f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}