{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPSEe/YSa/Ii5qW5scZGgMd"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0-cd3nxbybjG","executionInfo":{"status":"ok","timestamp":1689165978105,"user_tz":-360,"elapsed":904564,"user":{"displayName":"Tanmoy Mazumder","userId":"07166002203080382723"}},"outputId":"02f4fd43-fedc-4b90-9e98-27f8a3b28486"},"outputs":[{"output_type":"stream","name":"stdout","text":["Total Characters:  144683\n","Total Vocab:  49\n","Total Patterns:  144583\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 1/50: 100%|███████████████████| 565/565 [00:12<00:00, 43.60it/s, loss=595]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 0: Cross-entropy: 426854.7812\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 2/50: 100%|███████████████████| 565/565 [00:12<00:00, 43.96it/s, loss=572]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Cross-entropy: 403909.2812\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 3/50: 100%|███████████████████| 565/565 [00:13<00:00, 43.45it/s, loss=562]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2: Cross-entropy: 391075.8750\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 4/50: 100%|███████████████████| 565/565 [00:13<00:00, 42.62it/s, loss=528]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3: Cross-entropy: 382121.9375\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 5/50: 100%|███████████████████| 565/565 [00:14<00:00, 39.99it/s, loss=509]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 4: Cross-entropy: 374673.6562\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 6/50: 100%|███████████████████| 565/565 [00:13<00:00, 41.59it/s, loss=535]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 5: Cross-entropy: 365765.0625\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 7/50: 100%|███████████████████| 565/565 [00:13<00:00, 42.25it/s, loss=505]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 6: Cross-entropy: 358826.1875\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 8/50: 100%|███████████████████| 565/565 [00:13<00:00, 42.64it/s, loss=476]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 7: Cross-entropy: 350310.7500\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 9/50: 100%|███████████████████| 565/565 [00:13<00:00, 41.87it/s, loss=474]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 8: Cross-entropy: 342942.5625\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 10/50: 100%|██████████████████| 565/565 [00:13<00:00, 41.67it/s, loss=490]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 9: Cross-entropy: 336938.5938\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 11/50: 100%|██████████████████| 565/565 [00:13<00:00, 42.55it/s, loss=457]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 10: Cross-entropy: 330695.5938\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 12/50: 100%|██████████████████| 565/565 [00:13<00:00, 42.56it/s, loss=475]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 11: Cross-entropy: 325210.2812\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 13/50: 100%|██████████████████| 565/565 [00:13<00:00, 41.99it/s, loss=436]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 12: Cross-entropy: 318751.3438\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 14/50: 100%|██████████████████| 565/565 [00:13<00:00, 40.97it/s, loss=432]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 13: Cross-entropy: 314261.1250\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 15/50: 100%|██████████████████| 565/565 [00:13<00:00, 42.01it/s, loss=437]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 14: Cross-entropy: 308449.5312\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 16/50: 100%|██████████████████| 565/565 [00:13<00:00, 41.67it/s, loss=420]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 15: Cross-entropy: 302686.6562\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 17/50: 100%|██████████████████| 565/565 [00:13<00:00, 41.38it/s, loss=430]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 16: Cross-entropy: 298673.0312\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 18/50: 100%|██████████████████| 565/565 [00:13<00:00, 41.80it/s, loss=425]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 17: Cross-entropy: 293267.4688\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 19/50: 100%|██████████████████| 565/565 [00:13<00:00, 41.59it/s, loss=432]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 18: Cross-entropy: 289345.9062\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 20/50: 100%|██████████████████| 565/565 [00:13<00:00, 41.78it/s, loss=415]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 19: Cross-entropy: 284572.5938\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 21/50: 100%|██████████████████| 565/565 [00:13<00:00, 40.60it/s, loss=377]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 20: Cross-entropy: 279087.2812\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 22/50: 100%|██████████████████| 565/565 [00:13<00:00, 42.16it/s, loss=444]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 21: Cross-entropy: 276465.8750\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 23/50: 100%|██████████████████| 565/565 [00:13<00:00, 42.02it/s, loss=409]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 22: Cross-entropy: 272151.0000\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 24/50: 100%|██████████████████| 565/565 [00:13<00:00, 41.98it/s, loss=412]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 23: Cross-entropy: 268422.8438\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 25/50: 100%|██████████████████| 565/565 [00:13<00:00, 41.30it/s, loss=403]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 24: Cross-entropy: 264836.5625\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 26/50: 100%|██████████████████| 565/565 [00:13<00:00, 41.96it/s, loss=408]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 25: Cross-entropy: 263329.4688\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 27/50: 100%|██████████████████| 565/565 [00:13<00:00, 41.99it/s, loss=373]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 26: Cross-entropy: 257346.4688\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 28/50: 100%|██████████████████| 565/565 [00:13<00:00, 42.37it/s, loss=374]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 27: Cross-entropy: 254523.4688\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 29/50: 100%|██████████████████| 565/565 [00:13<00:00, 41.72it/s, loss=369]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 28: Cross-entropy: 252341.7656\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 30/50: 100%|██████████████████| 565/565 [00:13<00:00, 42.14it/s, loss=395]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 29: Cross-entropy: 248639.6719\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 31/50: 100%|██████████████████| 565/565 [00:13<00:00, 42.47it/s, loss=342]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 30: Cross-entropy: 247495.7344\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 32/50: 100%|██████████████████| 565/565 [00:13<00:00, 41.83it/s, loss=363]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 31: Cross-entropy: 244068.3438\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 33/50: 100%|██████████████████| 565/565 [00:13<00:00, 41.64it/s, loss=379]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 32: Cross-entropy: 242632.9531\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 34/50: 100%|██████████████████| 565/565 [00:13<00:00, 42.37it/s, loss=354]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 33: Cross-entropy: 237857.7344\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 35/50: 100%|██████████████████| 565/565 [00:13<00:00, 42.18it/s, loss=391]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 34: Cross-entropy: 236458.7031\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 36/50: 100%|██████████████████| 565/565 [00:13<00:00, 41.24it/s, loss=326]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 35: Cross-entropy: 235337.8750\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 37/50: 100%|██████████████████| 565/565 [00:13<00:00, 40.95it/s, loss=326]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 36: Cross-entropy: 234763.7656\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 38/50: 100%|██████████████████| 565/565 [00:13<00:00, 42.18it/s, loss=342]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 37: Cross-entropy: 230894.2656\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 39/50: 100%|██████████████████| 565/565 [00:13<00:00, 42.25it/s, loss=399]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 38: Cross-entropy: 229638.5156\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 40/50: 100%|██████████████████| 565/565 [00:13<00:00, 41.97it/s, loss=360]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 39: Cross-entropy: 226715.9844\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 41/50: 100%|██████████████████| 565/565 [00:13<00:00, 41.87it/s, loss=368]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 40: Cross-entropy: 224310.3281\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 42/50: 100%|██████████████████| 565/565 [00:13<00:00, 42.29it/s, loss=321]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 41: Cross-entropy: 222936.1250\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 43/50: 100%|██████████████████| 565/565 [00:13<00:00, 42.04it/s, loss=353]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 42: Cross-entropy: 221504.2656\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 44/50: 100%|██████████████████| 565/565 [00:13<00:00, 41.67it/s, loss=369]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 43: Cross-entropy: 222718.9062\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 45/50: 100%|██████████████████| 565/565 [00:13<00:00, 41.68it/s, loss=312]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 44: Cross-entropy: 220289.9531\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 46/50: 100%|██████████████████| 565/565 [00:13<00:00, 41.89it/s, loss=323]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 45: Cross-entropy: 216972.5781\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 47/50: 100%|██████████████████| 565/565 [00:13<00:00, 42.33it/s, loss=339]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 46: Cross-entropy: 216845.5000\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 48/50: 100%|██████████████████| 565/565 [00:13<00:00, 41.48it/s, loss=286]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 47: Cross-entropy: 214628.2500\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 49/50: 100%|██████████████████| 565/565 [00:13<00:00, 42.35it/s, loss=335]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 48: Cross-entropy: 211298.2031\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 50/50: 100%|██████████████████| 565/565 [00:13<00:00, 42.41it/s, loss=304]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 49: Cross-entropy: 211132.0469\n","Prompt: \"tes on their slates, and then added them up, and\n","reduced the answer to shillings and pence.\n","\n","“take o\"\n","t hotingte they sares,” the macch hare tooek te senting aotiously, “iot then the roeere of the soiat ” the said to herself, “it would be a genter, iewsh  be iore  a  arocoss make oe an the that sien ied th the ioeo hareer an iers,”\n","\n","“i shanl that you toul to wour hott the dorcernonn,” said the mock turtle. \n","“toe mooh a dutious toond?”,” said alice,\n","“io you doo’t keke the sooe of the doerest. io an solerfing arouoe_”\n","\n","“hu wes i than tou doote,” she match hare in a soee aunroe ooeere to ce taid io a toie oong toye, and whin she wasted to tee it tu ho the wan ohe whsh she white rabbit, and sas atin inooing so ce tith tee hoare of the sohde  sie was a little thar soret in the looken to her ane, an the was to toenk the oockr oaddite the roees shat sas the winle gar ane the errphon sad ano thet was the winle gar oo a sirugls  shate tas aoi toeezing so tay then ali mhrely, and wound oo aoa the war to tey to the haad, and the was qeaking at the could, “it would ba ko keed that iote thet monent\n","Done.\n"]}],"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.utils.data as data\n","from tqdm import tqdm\n","\n","# load ascii text and convert to lowercase\n","filename = \"wonderland.txt\"\n","raw_text = open(filename, 'r', encoding='utf-8').read()\n","raw_text = raw_text.lower()\n","\n","# create mapping of unique chars to integers\n","chars = sorted(list(set(raw_text)))\n","char_to_int = dict((c, i) for i, c in enumerate(chars))\n","\n","# summarize the loaded data\n","n_chars = len(raw_text)\n","n_vocab = len(chars)\n","print(\"Total Characters: \", n_chars)\n","print(\"Total Vocab: \", n_vocab)\n","\n","# prepare the dataset of input to output pairs encoded as integers\n","seq_length = 100\n","dataX = []\n","dataY = []\n","for i in range(0, n_chars - seq_length, 1):\n","    seq_in = raw_text[i:i + seq_length]\n","    seq_out = raw_text[i + seq_length]\n","    dataX.append([char_to_int[char] for char in seq_in])\n","    dataY.append(char_to_int[seq_out])\n","n_patterns = len(dataX)\n","print(\"Total Patterns: \", n_patterns)\n","\n","# reshape X to be [samples, time steps, features]\n","X = torch.tensor(dataX, dtype=torch.float32).reshape(n_patterns, seq_length, 1)\n","X = X / float(n_vocab)\n","y = torch.tensor(dataY)\n","\n","class CharModel(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.lstm = nn.LSTM(input_size=1, hidden_size=256, num_layers=1)\n","        self.dropout = nn.Dropout(0.2)\n","        self.linear = nn.Linear(256, n_vocab)\n","    def forward(self, x):\n","        x, _ = self.lstm(x)\n","        # take only the last output\n","        x = x[:, -1, :]\n","        # produce output\n","        x = self.linear(self.dropout(x))\n","        return x\n","\n","n_epochs = 50\n","batch_size = 256 #128\n","model = CharModel()\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n","optimizer = optim.Adam(model.parameters())\n","loss_fn = nn.CrossEntropyLoss(reduction=\"sum\")\n","loader = data.DataLoader(data.TensorDataset(X, y), shuffle=True, batch_size=batch_size)\n","\n","best_model = None\n","best_loss = np.inf\n","for epoch in range(n_epochs):\n","    model.train()\n","    with tqdm(total=len(loader), ncols=80, desc=f\"Epoch {epoch+1}/{n_epochs}\") as pbar:\n","        for X_batch, y_batch in loader:\n","            y_pred = model(X_batch.to(device))\n","            loss = loss_fn(y_pred, y_batch.to(device))\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","            pbar.set_postfix(loss=loss.item())\n","            pbar.update()\n","    # Validation\n","    model.eval()\n","    loss = 0\n","    with torch.no_grad():\n","        for X_batch, y_batch in loader:\n","            y_pred = model(X_batch.to(device))\n","            loss += loss_fn(y_pred, y_batch.to(device))\n","        if loss < best_loss:\n","            best_loss = loss\n","            best_model = model.state_dict()\n","        print(\"Epoch %d: Cross-entropy: %.4f\" % (epoch, loss))\n","\n","torch.save([best_model, char_to_int], \"single-char.pth\")\n","\n","# Generation using the trained model\n","best_model, char_to_int = torch.load(\"single-char.pth\")\n","n_vocab = len(char_to_int)\n","int_to_char = dict((i, c) for c, i in char_to_int.items())\n","model.load_state_dict(best_model)\n","\n","# randomly generate a prompt\n","filename = \"wonderland.txt\"\n","seq_length = 100\n","raw_text = open(filename, 'r', encoding='utf-8').read()\n","raw_text = raw_text.lower()\n","start = np.random.randint(0, len(raw_text)-seq_length)\n","prompt = raw_text[start:start+seq_length]\n","pattern = [char_to_int[c] for c in prompt]\n","\n","model.eval()\n","print('Prompt: \"%s\"' % prompt)\n","with torch.no_grad():\n","    for i in range(1000):\n","        # format input array of int into PyTorch tensor\n","        x = np.reshape(pattern, (1, len(pattern), 1)) / float(n_vocab)\n","        x = torch.tensor(x, dtype=torch.float32)\n","        # generate logits as output from the model\n","        prediction = model(x.to(device))\n","        # convert logits into one character\n","        index = int(prediction.argmax())\n","        result = int_to_char[index]\n","        print(result, end=\"\")\n","        # append the new character into the prompt for the next iteration\n","        pattern.append(index)\n","        pattern = pattern[1:]\n","print()\n","print(\"Done.\")\n"]}]}