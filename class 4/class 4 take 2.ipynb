{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyN9tUx3MOMSa6OAHYoDOjLc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O4XPHyCJ7_0s","executionInfo":{"status":"ok","timestamp":1689168571859,"user_tz":-360,"elapsed":1919603,"user":{"displayName":"Tanmoy Mazumder","userId":"07166002203080382723"}},"outputId":"4f1a8e3a-8003-4326-e082-4ce7f124642a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Total Characters:  144683\n","Total Vocab:  49\n","Total Patterns:  144583\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 1/50: 100%|███████████████████| 565/565 [00:27<00:00, 20.54it/s, loss=551]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 0: Cross-entropy: 404953.3750\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 2/50: 100%|███████████████████| 565/565 [00:26<00:00, 20.93it/s, loss=473]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1: Cross-entropy: 365744.1562\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 3/50: 100%|███████████████████| 565/565 [00:28<00:00, 20.06it/s, loss=472]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2: Cross-entropy: 341380.2188\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 4/50: 100%|███████████████████| 565/565 [00:29<00:00, 19.44it/s, loss=466]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3: Cross-entropy: 323552.6875\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 5/50: 100%|███████████████████| 565/565 [00:28<00:00, 19.75it/s, loss=420]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 4: Cross-entropy: 306129.3125\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 6/50: 100%|███████████████████| 565/565 [00:28<00:00, 19.69it/s, loss=469]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 5: Cross-entropy: 293979.6562\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 7/50: 100%|███████████████████| 565/565 [00:28<00:00, 19.56it/s, loss=427]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 6: Cross-entropy: 284113.6250\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 8/50: 100%|███████████████████| 565/565 [00:28<00:00, 19.68it/s, loss=424]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 7: Cross-entropy: 279338.6875\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 9/50: 100%|███████████████████| 565/565 [00:28<00:00, 19.73it/s, loss=408]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 8: Cross-entropy: 267653.9062\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 10/50: 100%|██████████████████| 565/565 [00:28<00:00, 19.70it/s, loss=378]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 9: Cross-entropy: 259680.7344\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 11/50: 100%|██████████████████| 565/565 [00:28<00:00, 19.59it/s, loss=337]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 10: Cross-entropy: 253961.8438\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 12/50: 100%|██████████████████| 565/565 [00:28<00:00, 19.72it/s, loss=375]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 11: Cross-entropy: 248642.7344\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 13/50: 100%|██████████████████| 565/565 [00:28<00:00, 19.71it/s, loss=357]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 12: Cross-entropy: 243449.8906\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 14/50: 100%|██████████████████| 565/565 [00:28<00:00, 19.58it/s, loss=329]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 13: Cross-entropy: 238102.6875\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 15/50: 100%|██████████████████| 565/565 [00:29<00:00, 19.46it/s, loss=366]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 14: Cross-entropy: 242938.1406\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 16/50: 100%|██████████████████| 565/565 [00:28<00:00, 19.70it/s, loss=342]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 15: Cross-entropy: 228198.4688\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 17/50: 100%|██████████████████| 565/565 [00:28<00:00, 19.65it/s, loss=313]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 16: Cross-entropy: 224648.8281\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 18/50: 100%|██████████████████| 565/565 [00:29<00:00, 19.46it/s, loss=389]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 17: Cross-entropy: 221821.9844\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 19/50: 100%|██████████████████| 565/565 [00:28<00:00, 19.52it/s, loss=361]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 18: Cross-entropy: 218658.2812\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 20/50: 100%|██████████████████| 565/565 [00:28<00:00, 19.63it/s, loss=354]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 19: Cross-entropy: 216991.0000\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 21/50: 100%|██████████████████| 565/565 [00:28<00:00, 19.71it/s, loss=329]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 20: Cross-entropy: 211234.5156\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 22/50: 100%|██████████████████| 565/565 [00:28<00:00, 19.59it/s, loss=313]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 21: Cross-entropy: 206682.0312\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 23/50: 100%|██████████████████| 565/565 [00:28<00:00, 19.72it/s, loss=288]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 22: Cross-entropy: 204965.0156\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 24/50: 100%|██████████████████| 565/565 [00:28<00:00, 19.64it/s, loss=314]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 23: Cross-entropy: 200147.7500\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 25/50: 100%|██████████████████| 565/565 [00:28<00:00, 19.67it/s, loss=326]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 24: Cross-entropy: 207341.5312\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 26/50: 100%|██████████████████| 565/565 [00:29<00:00, 19.39it/s, loss=281]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 25: Cross-entropy: 196540.0000\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 27/50: 100%|██████████████████| 565/565 [00:28<00:00, 19.57it/s, loss=339]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 26: Cross-entropy: 193277.7969\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 28/50: 100%|██████████████████| 565/565 [00:28<00:00, 19.71it/s, loss=317]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 27: Cross-entropy: 190504.3906\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 29/50: 100%|██████████████████| 565/565 [00:28<00:00, 19.60it/s, loss=319]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 28: Cross-entropy: 189462.3125\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 30/50: 100%|██████████████████| 565/565 [00:28<00:00, 19.57it/s, loss=294]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 29: Cross-entropy: 186541.4219\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 31/50: 100%|██████████████████| 565/565 [00:28<00:00, 19.70it/s, loss=304]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 30: Cross-entropy: 182533.7812\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 32/50: 100%|██████████████████| 565/565 [00:28<00:00, 19.72it/s, loss=290]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 31: Cross-entropy: 181056.8125\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 33/50: 100%|██████████████████| 565/565 [00:28<00:00, 19.56it/s, loss=320]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 32: Cross-entropy: 178029.7031\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 34/50: 100%|██████████████████| 565/565 [00:28<00:00, 19.60it/s, loss=315]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 33: Cross-entropy: 175775.6250\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 35/50: 100%|██████████████████| 565/565 [00:28<00:00, 19.67it/s, loss=297]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 34: Cross-entropy: 173377.8438\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 36/50: 100%|██████████████████| 565/565 [00:28<00:00, 19.64it/s, loss=300]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 35: Cross-entropy: 172337.1562\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 37/50: 100%|██████████████████| 565/565 [00:28<00:00, 19.53it/s, loss=281]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 36: Cross-entropy: 170570.5938\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 38/50: 100%|██████████████████| 565/565 [00:29<00:00, 19.42it/s, loss=278]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 37: Cross-entropy: 166922.2812\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 39/50: 100%|██████████████████| 565/565 [00:28<00:00, 19.57it/s, loss=243]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 38: Cross-entropy: 167911.0781\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 40/50: 100%|██████████████████| 565/565 [00:28<00:00, 19.63it/s, loss=266]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 39: Cross-entropy: 164938.6719\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 41/50: 100%|██████████████████| 565/565 [00:28<00:00, 19.52it/s, loss=264]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 40: Cross-entropy: 163186.6406\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 42/50: 100%|██████████████████| 565/565 [00:28<00:00, 19.70it/s, loss=286]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 41: Cross-entropy: 161840.0781\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 43/50: 100%|██████████████████| 565/565 [00:28<00:00, 19.63it/s, loss=281]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 42: Cross-entropy: 158969.8594\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 44/50: 100%|██████████████████| 565/565 [00:28<00:00, 19.65it/s, loss=264]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 43: Cross-entropy: 158120.1875\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 45/50: 100%|██████████████████| 565/565 [00:28<00:00, 19.51it/s, loss=250]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 44: Cross-entropy: 157336.5312\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 46/50: 100%|██████████████████| 565/565 [00:28<00:00, 19.67it/s, loss=298]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 45: Cross-entropy: 156125.5938\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 47/50: 100%|██████████████████| 565/565 [00:28<00:00, 19.67it/s, loss=288]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 46: Cross-entropy: 153015.8281\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 48/50: 100%|██████████████████| 565/565 [00:28<00:00, 19.50it/s, loss=251]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 47: Cross-entropy: 151338.9688\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 49/50: 100%|██████████████████| 565/565 [00:29<00:00, 19.45it/s, loss=283]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 48: Cross-entropy: 150973.6719\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 50/50: 100%|██████████████████| 565/565 [00:28<00:00, 19.66it/s, loss=293]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 49: Cross-entropy: 150519.3281\n"]},{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":2}],"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.utils.data as data\n","from tqdm import tqdm\n","\n","# load ascii text and convert to lowercase\n","filename = \"wonderland.txt\"\n","raw_text = open(filename, 'r', encoding='utf-8').read()\n","raw_text = raw_text.lower()\n","\n","# create mapping of unique chars to integers\n","chars = sorted(list(set(raw_text)))\n","char_to_int = dict((c, i) for i, c in enumerate(chars))\n","\n","# summarize the loaded data\n","n_chars = len(raw_text)\n","n_vocab = len(chars)\n","print(\"Total Characters: \", n_chars)\n","print(\"Total Vocab: \", n_vocab)\n","\n","# prepare the dataset of input to output pairs encoded as integers\n","seq_length = 100\n","dataX = []\n","dataY = []\n","for i in range(0, n_chars - seq_length, 1):\n","    seq_in = raw_text[i:i + seq_length]\n","    seq_out = raw_text[i + seq_length]\n","    dataX.append([char_to_int[char] for char in seq_in])\n","    dataY.append(char_to_int[seq_out])\n","n_patterns = len(dataX)\n","print(\"Total Patterns: \", n_patterns)\n","\n","# reshape X to be [samples, time steps, features]\n","X = torch.tensor(dataX, dtype=torch.float32).reshape(n_patterns, seq_length, 1)\n","X = X / float(n_vocab)\n","y = torch.tensor(dataY)\n","\n","class CharModel(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.lstm = nn.LSTM(input_size=1, hidden_size=256, num_layers=2, dropout=0.2)\n","        self.dropout = nn.Dropout(0.2)\n","        self.linear = nn.Linear(256, n_vocab)\n","    def forward(self, x):\n","        x, _ = self.lstm(x)\n","        # take only the last output\n","        x = x[:, -1, :]\n","        # produce output\n","        x = self.linear(self.dropout(x))\n","        return x\n","\n","n_epochs = 50\n","batch_size = 256 # 128\n","model = CharModel()\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n","optimizer = optim.Adam(model.parameters())\n","loss_fn = nn.CrossEntropyLoss(reduction=\"sum\")\n","loader = data.DataLoader(data.TensorDataset(X, y), shuffle=True, batch_size=batch_size)\n","\n","best_model = None\n","best_loss = np.inf\n","for epoch in range(n_epochs):\n","    model.train()\n","    with tqdm(total=len(loader), ncols=80, desc=f\"Epoch {epoch+1}/{n_epochs}\") as pbar:\n","        for X_batch, y_batch in loader:\n","            y_pred = model(X_batch.to(device))\n","            loss = loss_fn(y_pred, y_batch.to(device))\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","            pbar.set_postfix(loss=loss.item())\n","            pbar.update()\n","    # Validation\n","    model.eval()\n","    loss = 0\n","    with torch.no_grad():\n","        for X_batch, y_batch in loader:\n","            y_pred = model(X_batch.to(device))\n","            loss += loss_fn(y_pred, y_batch.to(device))\n","        if loss < best_loss:\n","            best_loss = loss\n","            best_model = model.state_dict()\n","        print(\"Epoch %d: Cross-entropy: %.4f\" % (epoch, loss))\n","\n","torch.save([best_model, char_to_int], \"single-char.pth\")\n","\n","# Generation using the trained model\n","best_model, char_to_int = torch.load(\"single-char.pth\")\n","n_vocab = len(char_to_int)\n","int_to_char = dict((i, c) for c, i in char_to_int.items())\n","model.load_state_dict(best_model)\n","\n"]},{"cell_type":"code","source":["# randomly generate a prompt\n","filename = \"wonderland.txt\"\n","seq_length = 100\n","raw_text = open(filename, 'r', encoding='utf-8').read()\n","raw_text = raw_text.lower()\n","start = np.random.randint(0, len(raw_text)-seq_length)\n","prompt = raw_text[start:start+seq_length]\n","pattern = [char_to_int[c] for c in prompt]\n","\n","model.eval()\n","print('Prompt: \"%s\"' % prompt)\n","with torch.no_grad():\n","    for i in range(1000):\n","        # format input array of int into PyTorch tensor\n","        x = np.reshape(pattern, (1, len(pattern), 1)) / float(n_vocab)\n","        x = torch.tensor(x, dtype=torch.float32)\n","        # generate logits as output from the model\n","        prediction = model(x.to(device))\n","        # convert logits into one character\n","        index = int(prediction.argmax())\n","        result = int_to_char[index]\n","        print(result, end=\"\")\n","        # append the new character into the prompt for the next iteration\n","        pattern.append(index)\n","        pattern = pattern[1:]\n","print()\n","print(\"Done.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wwhYtYQGLWRw","executionInfo":{"status":"ok","timestamp":1689169096645,"user_tz":-360,"elapsed":3311,"user":{"displayName":"Tanmoy Mazumder","userId":"07166002203080382723"}},"outputId":"f20f71c1-7044-4c85-98a5-3fa36272a37a"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Prompt: \"on your shoes and stockings for you now, dears? i’m\n","sure _i_ shan’t be able! i shall be a great deal\"\n"," to toear as the sab. she was a little forn to the that?”\n","\n","“i don’t know the way the white rabbit with the same shings and surping it to sell you think i can do br the could,\n","\n","“nh, i wiol you may soe better nine a little for to thin,” she said to herself, “it would be a little wiile the borts of that is would be a little foor to the thather, and the mouse doole the had goown to her freat size. the was a little wirless all the way of sarier and sale the fiange of the court, and the shget seale in a low voice. \n","“what _is_ the same thing is the semark the sea,” the mock turtle seplied in a low voice.\n","\n","“what _is_ the same thing is the semark the sea,” the mock turtle seplied in a low voice.\n","\n","“what _is_ the same thing is the semark the sea,” the mock turtle seplied in a low voice.\n","\n","“what _is_ the same thing is the semark the sea,” the mock turtle seplied in a low voice.\n","\n","“what _is_ the same thing is the semark the sea,” the mock turtle seplied in a low voice.\n","\n","“what _is_ the same thing is t\n","Done.\n"]}]}]}